{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Analyse der Banknoten-Authentifizierung mittels Clustering\n",
    "\n",
    "**Autor:** [Dein Name]\n",
    "**Kurs:** Applied AI I - Week 7 Assignment\n",
    "\n",
    "## 1. Einführung & Forschungsfrage\n",
    "In diesem Notebook untersuchen wir den \"Banknote Authentication Data Set\". Der Datensatz enthält Merkmale, die aus Bildern von echten und gefälschten Banknoten extrahiert wurden (Wavelet Transform Tools).\n",
    "\n",
    "**Forschungsfrage:**\n",
    "> *Können unüberwachte Lernalgorithmen (Unsupervised Learning) die zugrunde liegende Struktur von echten vs. gefälschten Banknoten ohne Labels wiedererkennen? Sind die gefundenen Cluster interpretierbar?*"
   ],
   "id": "12dc1fa25a0186c"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-23T13:38:35.400949Z",
     "start_time": "2025-11-23T13:38:35.262447Z"
    }
   },
   "source": [
    "# -------- IMPORTS --------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import (silhouette_score, davies_bouldin_score,\n",
    "                             calinski_harabasz_score, adjusted_rand_score,\n",
    "                             normalized_mutual_info_score, homogeneity_score,\n",
    "                             completeness_score, v_measure_score)\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"✓ All libraries imported successfully!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All libraries imported successfully!\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. LOAD DATA (Banknote Authentication)\n",
    "Link: https://archive.ics.uci.edu/dataset/267/banknote+authentication"
   ],
   "id": "97fda32fec8ab5ef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T13:38:36.104289Z",
     "start_time": "2025-11-23T13:38:35.405006Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ============================================\n",
    "# 1. Load Data\n",
    "# ============================================\n",
    "\n",
    "# Definition der Spaltennamen gemäß UCI Dokumentation\n",
    "column_names = ['variance', 'skewness', 'curtosis', 'entropy', 'class']\n",
    "\n",
    "# Laden der Daten direkt vom UCI Repository\n",
    "#url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt\"\n",
    "#df = pd.read_csv(url, names=column_names, header=None)\n",
    "\n",
    "# fetch dataset\n",
    "banknote_authentication = fetch_ucirepo(id=267)\n",
    "\n",
    "# data (as pandas dataframes)\n",
    "X = banknote_authentication.data.features\n",
    "y = banknote_authentication.data.targets\n",
    "\n",
    "# Combine for easier manipulation\n",
    "df = pd.concat([X, y], axis=1)\n",
    "\n",
    "print(f\"Dataset loaded successfully!\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "display(df.head())"
   ],
   "id": "e3d90804b2186abb",
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "Error connecting to server",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mSSLCertVerificationError\u001B[39m                  Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/urllib/request.py:1348\u001B[39m, in \u001B[36mAbstractHTTPHandler.do_open\u001B[39m\u001B[34m(self, http_class, req, **http_conn_args)\u001B[39m\n\u001B[32m   1347\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1348\u001B[39m     \u001B[43mh\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreq\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreq\u001B[49m\u001B[43m.\u001B[49m\u001B[43mselector\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreq\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1349\u001B[39m \u001B[43m              \u001B[49m\u001B[43mencode_chunked\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreq\u001B[49m\u001B[43m.\u001B[49m\u001B[43mhas_header\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mTransfer-encoding\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1350\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err: \u001B[38;5;66;03m# timeout error\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:1303\u001B[39m, in \u001B[36mHTTPConnection.request\u001B[39m\u001B[34m(self, method, url, body, headers, encode_chunked)\u001B[39m\n\u001B[32m   1302\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Send a complete request to the server.\"\"\"\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1303\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_send_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencode_chunked\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:1349\u001B[39m, in \u001B[36mHTTPConnection._send_request\u001B[39m\u001B[34m(self, method, url, body, headers, encode_chunked)\u001B[39m\n\u001B[32m   1348\u001B[39m     body = _encode(body, \u001B[33m'\u001B[39m\u001B[33mbody\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m-> \u001B[39m\u001B[32m1349\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mendheaders\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencode_chunked\u001B[49m\u001B[43m=\u001B[49m\u001B[43mencode_chunked\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:1298\u001B[39m, in \u001B[36mHTTPConnection.endheaders\u001B[39m\u001B[34m(self, message_body, encode_chunked)\u001B[39m\n\u001B[32m   1297\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m CannotSendHeader()\n\u001B[32m-> \u001B[39m\u001B[32m1298\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_send_output\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmessage_body\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencode_chunked\u001B[49m\u001B[43m=\u001B[49m\u001B[43mencode_chunked\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:1058\u001B[39m, in \u001B[36mHTTPConnection._send_output\u001B[39m\u001B[34m(self, message_body, encode_chunked)\u001B[39m\n\u001B[32m   1057\u001B[39m \u001B[38;5;28;01mdel\u001B[39;00m \u001B[38;5;28mself\u001B[39m._buffer[:]\n\u001B[32m-> \u001B[39m\u001B[32m1058\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmsg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1060\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m message_body \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   1061\u001B[39m \n\u001B[32m   1062\u001B[39m     \u001B[38;5;66;03m# create a consistent interface to message_body\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:996\u001B[39m, in \u001B[36mHTTPConnection.send\u001B[39m\u001B[34m(self, data)\u001B[39m\n\u001B[32m    995\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.auto_open:\n\u001B[32m--> \u001B[39m\u001B[32m996\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mconnect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    997\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:1475\u001B[39m, in \u001B[36mHTTPSConnection.connect\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1473\u001B[39m     server_hostname = \u001B[38;5;28mself\u001B[39m.host\n\u001B[32m-> \u001B[39m\u001B[32m1475\u001B[39m \u001B[38;5;28mself\u001B[39m.sock = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_context\u001B[49m\u001B[43m.\u001B[49m\u001B[43mwrap_socket\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msock\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1476\u001B[39m \u001B[43m                                      \u001B[49m\u001B[43mserver_hostname\u001B[49m\u001B[43m=\u001B[49m\u001B[43mserver_hostname\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py:517\u001B[39m, in \u001B[36mSSLContext.wrap_socket\u001B[39m\u001B[34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001B[39m\n\u001B[32m    511\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mwrap_socket\u001B[39m(\u001B[38;5;28mself\u001B[39m, sock, server_side=\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[32m    512\u001B[39m                 do_handshake_on_connect=\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[32m    513\u001B[39m                 suppress_ragged_eofs=\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[32m    514\u001B[39m                 server_hostname=\u001B[38;5;28;01mNone\u001B[39;00m, session=\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[32m    515\u001B[39m     \u001B[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001B[39;00m\n\u001B[32m    516\u001B[39m     \u001B[38;5;66;03m# ctx._wrap_socket()\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m517\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msslsocket_class\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_create\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    518\u001B[39m \u001B[43m        \u001B[49m\u001B[43msock\u001B[49m\u001B[43m=\u001B[49m\u001B[43msock\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    519\u001B[39m \u001B[43m        \u001B[49m\u001B[43mserver_side\u001B[49m\u001B[43m=\u001B[49m\u001B[43mserver_side\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    520\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdo_handshake_on_connect\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdo_handshake_on_connect\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    521\u001B[39m \u001B[43m        \u001B[49m\u001B[43msuppress_ragged_eofs\u001B[49m\u001B[43m=\u001B[49m\u001B[43msuppress_ragged_eofs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    522\u001B[39m \u001B[43m        \u001B[49m\u001B[43mserver_hostname\u001B[49m\u001B[43m=\u001B[49m\u001B[43mserver_hostname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    523\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcontext\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    524\u001B[39m \u001B[43m        \u001B[49m\u001B[43msession\u001B[49m\u001B[43m=\u001B[49m\u001B[43msession\u001B[49m\n\u001B[32m    525\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py:1104\u001B[39m, in \u001B[36mSSLSocket._create\u001B[39m\u001B[34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001B[39m\n\u001B[32m   1103\u001B[39m                 \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mdo_handshake_on_connect should not be specified for non-blocking sockets\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m-> \u001B[39m\u001B[32m1104\u001B[39m             \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdo_handshake\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1105\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py:1382\u001B[39m, in \u001B[36mSSLSocket.do_handshake\u001B[39m\u001B[34m(self, block)\u001B[39m\n\u001B[32m   1381\u001B[39m         \u001B[38;5;28mself\u001B[39m.settimeout(\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[32m-> \u001B[39m\u001B[32m1382\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_sslobj\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdo_handshake\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1383\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
      "\u001B[31mSSLCertVerificationError\u001B[39m: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1006)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[31mURLError\u001B[39m                                  Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/ucimlrepo/fetch.py:68\u001B[39m, in \u001B[36mfetch_ucirepo\u001B[39m\u001B[34m(name, id)\u001B[39m\n\u001B[32m     67\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m68\u001B[39m     response = \u001B[43murllib\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m.\u001B[49m\u001B[43murlopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mapi_url\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcontext\u001B[49m\u001B[43m=\u001B[49m\u001B[43mssl\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcreate_default_context\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcafile\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcertifi\u001B[49m\u001B[43m.\u001B[49m\u001B[43mwhere\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     69\u001B[39m     data = json.load(response)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/urllib/request.py:216\u001B[39m, in \u001B[36murlopen\u001B[39m\u001B[34m(url, data, timeout, cafile, capath, cadefault, context)\u001B[39m\n\u001B[32m    215\u001B[39m     opener = _opener\n\u001B[32m--> \u001B[39m\u001B[32m216\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mopener\u001B[49m\u001B[43m.\u001B[49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/urllib/request.py:519\u001B[39m, in \u001B[36mOpenerDirector.open\u001B[39m\u001B[34m(self, fullurl, data, timeout)\u001B[39m\n\u001B[32m    518\u001B[39m sys.audit(\u001B[33m'\u001B[39m\u001B[33murllib.Request\u001B[39m\u001B[33m'\u001B[39m, req.full_url, req.data, req.headers, req.get_method())\n\u001B[32m--> \u001B[39m\u001B[32m519\u001B[39m response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    521\u001B[39m \u001B[38;5;66;03m# post-process response\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/urllib/request.py:536\u001B[39m, in \u001B[36mOpenerDirector._open\u001B[39m\u001B[34m(self, req, data)\u001B[39m\n\u001B[32m    535\u001B[39m protocol = req.type\n\u001B[32m--> \u001B[39m\u001B[32m536\u001B[39m result = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_chain\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mhandle_open\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprotocol\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprotocol\u001B[49m\u001B[43m \u001B[49m\u001B[43m+\u001B[49m\n\u001B[32m    537\u001B[39m \u001B[43m                          \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m_open\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreq\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    538\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m result:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/urllib/request.py:496\u001B[39m, in \u001B[36mOpenerDirector._call_chain\u001B[39m\u001B[34m(self, chain, kind, meth_name, *args)\u001B[39m\n\u001B[32m    495\u001B[39m func = \u001B[38;5;28mgetattr\u001B[39m(handler, meth_name)\n\u001B[32m--> \u001B[39m\u001B[32m496\u001B[39m result = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    497\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/urllib/request.py:1391\u001B[39m, in \u001B[36mHTTPSHandler.https_open\u001B[39m\u001B[34m(self, req)\u001B[39m\n\u001B[32m   1390\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mhttps_open\u001B[39m(\u001B[38;5;28mself\u001B[39m, req):\n\u001B[32m-> \u001B[39m\u001B[32m1391\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdo_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhttp\u001B[49m\u001B[43m.\u001B[49m\u001B[43mclient\u001B[49m\u001B[43m.\u001B[49m\u001B[43mHTTPSConnection\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreq\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1392\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcontext\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_context\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcheck_hostname\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_check_hostname\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/urllib/request.py:1351\u001B[39m, in \u001B[36mAbstractHTTPHandler.do_open\u001B[39m\u001B[34m(self, http_class, req, **http_conn_args)\u001B[39m\n\u001B[32m   1350\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err: \u001B[38;5;66;03m# timeout error\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1351\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m URLError(err)\n\u001B[32m   1352\u001B[39m r = h.getresponse()\n",
      "\u001B[31mURLError\u001B[39m: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1006)>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[31mConnectionError\u001B[39m                           Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 13\u001B[39m\n\u001B[32m      6\u001B[39m column_names = [\u001B[33m'\u001B[39m\u001B[33mvariance\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mskewness\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mcurtosis\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mentropy\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mclass\u001B[39m\u001B[33m'\u001B[39m]\n\u001B[32m      8\u001B[39m \u001B[38;5;66;03m# Laden der Daten direkt vom UCI Repository\u001B[39;00m\n\u001B[32m      9\u001B[39m \u001B[38;5;66;03m#url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt\"\u001B[39;00m\n\u001B[32m     10\u001B[39m \u001B[38;5;66;03m#df = pd.read_csv(url, names=column_names, header=None)\u001B[39;00m\n\u001B[32m     11\u001B[39m \n\u001B[32m     12\u001B[39m \u001B[38;5;66;03m# fetch dataset\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m13\u001B[39m banknote_authentication = \u001B[43mfetch_ucirepo\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mid\u001B[39;49m\u001B[43m=\u001B[49m\u001B[32;43m267\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     15\u001B[39m \u001B[38;5;66;03m# data (as pandas dataframes)\u001B[39;00m\n\u001B[32m     16\u001B[39m X = banknote_authentication.data.features\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/ucimlrepo/fetch.py:71\u001B[39m, in \u001B[36mfetch_ucirepo\u001B[39m\u001B[34m(name, id)\u001B[39m\n\u001B[32m     69\u001B[39m     data = json.load(response)\n\u001B[32m     70\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m (urllib.error.URLError, urllib.error.HTTPError):\n\u001B[32m---> \u001B[39m\u001B[32m71\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mConnectionError\u001B[39;00m(\u001B[33m'\u001B[39m\u001B[33mError connecting to server\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m     73\u001B[39m \u001B[38;5;66;03m# verify that dataset exists \u001B[39;00m\n\u001B[32m     74\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m data[\u001B[33m'\u001B[39m\u001B[33mstatus\u001B[39m\u001B[33m'\u001B[39m] != \u001B[32m200\u001B[39m:\n",
      "\u001B[31mConnectionError\u001B[39m: Error connecting to server"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Explorative Datenanalyse (EDA)",
   "id": "3b75bf7ad9e62dab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ============================================\n",
    "# 2. EXPLORATORY DATA ANALYSIS\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPLORATORY DATA ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Basic info\n",
    "print(\"\\n1. Dataset Info:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\n2. Statistical Summary:\")\n",
    "display(df.describe())\n",
    "\n",
    "print(\"\\n3. Missing Values:\")\n",
    "missing = df.isnull().sum()\n",
    "print(missing[missing > 0])\n",
    "if missing.sum() == 0:\n",
    "    print(\"No missing values found.\")\n",
    "\n",
    "print(\"\\n4. Class Distribution (Ground Truth):\")\n",
    "print(df['class'].value_counts())\n",
    "\n",
    "# Set aside the label for later validation\n",
    "# Class 0: Authentic, Class 1: Inauthentic\n",
    "y_true = df['class'].values\n",
    "print(f\"\\nGround truth labels shape: {y_true.shape}\")\n",
    "\n",
    "# Visualize features\n",
    "numerical_features = ['variance', 'skewness', 'curtosis', 'entropy']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Distribution of Banknote Features', fontsize=16, fontweight='bold')\n",
    "\n",
    "for idx, feature in enumerate(numerical_features):\n",
    "    row = idx // 2\n",
    "    col = idx % 2\n",
    "    sns.histplot(data=df, x=feature, hue='class', kde=True, ax=axes[row, col], palette='viridis')\n",
    "    axes[row, col].set_title(f'{feature} Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation matrix\n",
    "print(\"\\n5. Correlation Matrix:\")\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(df.corr(), annot=True, fmt='.2f', cmap='coolwarm', center=0)\n",
    "plt.title('Correlation Matrix', fontsize=14, fontweight='bold')\n",
    "plt.show()"
   ],
   "id": "19cef82d50cfe3fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Data Prepocessing",
   "id": "d11b99b6dedd59e1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ============================================\n",
    "# 3. DATA PREPROCESSING (NA + Duplicates + IQR)\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA PREPROCESSING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Create a working copy\n",
    "df_processed = df.copy()\n",
    "original_start_shape = df_processed.shape\n",
    "print(f\"Initial shape: {original_start_shape}\")\n",
    "\n",
    "# --- A. CHECK FOR MISSING VALUES ---\n",
    "print(\"\\n1. Missing Values Check:\")\n",
    "missing_count = df_processed.isnull().sum()\n",
    "if missing_count.sum() == 0:\n",
    "    print(\"✓ No missing values found.\")\n",
    "else:\n",
    "    print(f\"⚠ Found {missing_count.sum()} missing values. Dropping rows...\")\n",
    "    df_processed = df_processed.dropna()\n",
    "    print(f\"  New shape after dropping NA: {df_processed.shape}\")\n",
    "\n",
    "# --- B. CHECK FOR DUPLICATES (NEU) ---\n",
    "print(\"\\n2. Duplicate Check:\")\n",
    "duplicates_count = df_processed.duplicated().sum()\n",
    "if duplicates_count > 0:\n",
    "    print(f\"⚠ Found {duplicates_count} duplicate rows. Removing them...\")\n",
    "    df_processed.drop_duplicates(inplace=True)\n",
    "    # Reset Index ist wichtig nach dem Droppen\n",
    "    df_processed = df_processed.reset_index(drop=True)\n",
    "    print(f\"  New shape after removing duplicates: {df_processed.shape}\")\n",
    "else:\n",
    "    print(\"✓ No duplicate rows found.\")\n",
    "\n",
    "# --- C. OUTLIER REMOVAL (IQR METHOD) ---\n",
    "print(\"\\n3. Outlier Removal (IQR Method):\")\n",
    "shape_before_iqr = df_processed.shape\n",
    "\n",
    "# Nur numerische Spalten für IQR nutzen (ohne Class)\n",
    "numerical_cols = ['variance', 'skewness', 'curtosis', 'entropy']\n",
    "\n",
    "# Berechnung von Q1 (25%) und Q3 (75%)\n",
    "Q1 = df_processed[numerical_cols].quantile(0.25)\n",
    "Q3 = df_processed[numerical_cols].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Definition der Grenzen (Standardfaktor 1.5)\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Filterbedingung: Zeilen behalten, die KEINE Ausreißer sind\n",
    "condition = ~((df_processed[numerical_cols] < lower_bound) | (df_processed[numerical_cols] > upper_bound)).any(axis=1)\n",
    "\n",
    "df_clean = df_processed[condition]\n",
    "\n",
    "rows_removed_iqr = shape_before_iqr[0] - df_clean.shape[0]\n",
    "print(f\"  Rows removed by IQR: {rows_removed_iqr} ({rows_removed_iqr/shape_before_iqr[0]*100:.1f}%)\")\n",
    "print(f\"  New shape: {df_clean.shape}\")\n",
    "\n",
    "# Übernehme bereinigtes Dataset\n",
    "df_processed = df_clean.reset_index(drop=True)\n",
    "\n",
    "# --- D. PREPARE FEATURES & SCALING ---\n",
    "print(\"\\n4. Feature Preparation:\")\n",
    "\n",
    "X = df_processed.drop('class', axis=1)\n",
    "# WICHTIG: y_true aktualisieren, da wir Zeilen gelöscht haben!\n",
    "y_true = df_processed['class'].values\n",
    "\n",
    "feature_names = X.columns.tolist()\n",
    "print(f\"  Features: {feature_names}\")\n",
    "\n",
    "print(\"\\n5. Feature Scaling:\")\n",
    "print(\"  Using StandardScaler to normalize variance and skewness ranges...\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Convert back to DataFrame for convenience\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=feature_names)\n",
    "\n",
    "print(\"\\n\" + \"-\"*30)\n",
    "print(\"✓ Preprocessing complete!\")\n",
    "print(f\"Final dataset shape: {X_scaled_df.shape}\")\n",
    "total_removed = original_start_shape[0] - X_scaled_df.shape[0]\n",
    "print(f\"Total rows removed: {total_removed}\")\n",
    "print(f\"Updated Ground Truth labels shape: {y_true.shape}\")"
   ],
   "id": "c1c4689e107e2e54",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. K-Means - find best K",
   "id": "10c5b062f314e814"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ============================================\n",
    "# 4. K-MEANS EXPERIMENT\n",
    "# ============================================\n",
    "\n",
    "# Range of k to try\n",
    "k_range = range(2, 8)\n",
    "wcss = []\n",
    "silhouette_scores = []\n",
    "\n",
    "print(\"\\nTesting K-Means for k = 2 to 9...\")\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=RANDOM_STATE, n_init=10)\n",
    "    labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "    wcss.append(kmeans.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(X_scaled, labels))\n",
    "\n",
    "    print(f\"k={k}: Inertia={wcss[-1]:.1f}, Silhouette={silhouette_scores[-1]:.3f}\")\n",
    "\n",
    "# Plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Elbow Method\n",
    "axes[0].plot(k_range, wcss, 'bo-')\n",
    "axes[0].set_title('Elbow Method')\n",
    "axes[0].set_xlabel('Number of clusters k')\n",
    "axes[0].set_ylabel('Inertia')\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Silhouette Score\n",
    "axes[1].plot(k_range, silhouette_scores, 'go-')\n",
    "axes[1].set_title('Silhouette Score')\n",
    "axes[1].set_xlabel('Number of clusters k')\n",
    "axes[1].set_ylabel('Score')\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Select best K (oft ist k=2 hier am besten, aber wir lassen den Code entscheiden)\n",
    "optimal_k = k_range[np.argmax(silhouette_scores)]\n",
    "print(f\"\\nSuggested Optimal k: {optimal_k}\")"
   ],
   "id": "e7136f3e782386e3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5. K-Means Final Model",
   "id": "a8c712399041f9ca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ============================================\n",
    "# 5. K-MEANS FINAL MODEL\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"K-MEANS CLUSTERING WITH K={optimal_k}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train final K-Means model\n",
    "kmeans_final = KMeans(n_clusters=optimal_k, random_state=RANDOM_STATE, n_init=20)\n",
    "kmeans_labels = kmeans_final.fit_predict(X_scaled)\n",
    "\n",
    "# Calculate final metrics\n",
    "kmeans_silhouette = silhouette_score(X_scaled, kmeans_labels)\n",
    "kmeans_db = davies_bouldin_score(X_scaled, kmeans_labels)\n",
    "kmeans_ch = calinski_harabasz_score(X_scaled, kmeans_labels)\n",
    "\n",
    "print(f\"\\nK-Means Performance Metrics:\")\n",
    "print(f\"  Silhouette Score: {kmeans_silhouette:.4f}\")\n",
    "print(f\"  Davies-Bouldin Index: {kmeans_db:.4f}\")\n",
    "print(f\"  Calinski-Harabasz Score: {kmeans_ch:.2f}\")\n",
    "\n",
    "print(f\"\\nCluster Sizes:\")\n",
    "unique, counts = np.unique(kmeans_labels, return_counts=True)\n",
    "for cluster, count in zip(unique, counts):\n",
    "    print(f\"  Cluster {cluster}: {count} samples ({count/len(kmeans_labels)*100:.1f}%)\")"
   ],
   "id": "1c8a136dc28059b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6. HIERARCHICAL CLUSTERING",
   "id": "c05b422b2cacfb7a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ============================================\n",
    "# 6. HIERARCHICAL CLUSTERING\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT 2: HIERARCHICAL CLUSTERING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Da das Dataset klein ist, nutzen wir alle Daten statt nur ein Sample\n",
    "X_sample = X_scaled\n",
    "\n",
    "print(f\"\\nUsing full dataset ({len(X_sample)} samples) for dendrogram...\")\n",
    "\n",
    "linkage_methods = ['ward', 'complete', 'average']\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "fig.suptitle('Hierarchical Clustering: Dendrograms', fontsize=16)\n",
    "\n",
    "for idx, method in enumerate(linkage_methods):\n",
    "    # Calculate linkage\n",
    "    Z = linkage(X_sample, method=method)\n",
    "\n",
    "    # Plot\n",
    "    dendrogram(Z, ax=axes[idx], no_labels=True, truncate_mode='lastp', p=30)\n",
    "    axes[idx].set_title(f'{method.capitalize()} Linkage')\n",
    "    axes[idx].set_xlabel('Cluster Size / Sample Index')\n",
    "\n",
    "    # Fit model for metrics\n",
    "    hc = AgglomerativeClustering(n_clusters=optimal_k, linkage=method)\n",
    "    hl_labels = hc.fit_predict(X_scaled)\n",
    "    s_score = silhouette_score(X_scaled, hl_labels)\n",
    "    print(f\"{method.capitalize()} linkage Silhouette Score: {s_score:.3f}\")\n",
    "\n",
    "    # Save 'ward' labels for final comparison as it usually works best for euclidean\n",
    "    if method == 'ward':\n",
    "        hier_labels_final = hl_labels\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "57710af21b10f845",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 7. DBSCAN EXPERIMENT",
   "id": "463a465e1287266c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ============================================\n",
    "# 7. DBSCAN EXPERIMENT\n",
    "# ============================================\n",
    "\n",
    "# K-Distance Graph\n",
    "neighbors = NearestNeighbors(n_neighbors=5)\n",
    "neighbors_fit = neighbors.fit(X_scaled)\n",
    "distances, indices = neighbors_fit.kneighbors(X_scaled)\n",
    "distances = np.sort(distances[:, 4], axis=0)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(distances)\n",
    "plt.title('K-Distance Graph (Suche den \"Knick\")')\n",
    "plt.ylabel('Eps distance')\n",
    "plt.xlabel('Points sorted by distance')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"Testing DBSCAN parameters (Optimized search)...\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Params':<20} | {'Clusters':<8} | {'Noise %':<8} | {'Raw Sil':<8} | {'Adj. Score':<8}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# 1. FEINERE PARAMETER-ABSTUFUNG\n",
    "eps_values = [0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.6]\n",
    "min_samples_values = [5, 10, 15]\n",
    "\n",
    "eps_values = [0.5, 0.6, 0.75, 0.85, 1.0]\n",
    "min_samples_values = [5, 10, 15]\n",
    "\n",
    "best_score = -999\n",
    "best_dbscan_labels = None\n",
    "best_params = {}\n",
    "\n",
    "total_samples = len(X_scaled)\n",
    "\n",
    "for eps in eps_values:\n",
    "    for min_samples in min_samples_values:\n",
    "        db = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        labels = db.fit_predict(X_scaled)\n",
    "\n",
    "        # Maske für Nicht-Noise Punkte\n",
    "        mask = labels != -1\n",
    "        valid_labels = labels[mask]\n",
    "\n",
    "        n_clusters = len(set(valid_labels))\n",
    "        n_noise = list(labels).count(-1)\n",
    "        noise_ratio = n_noise / total_samples\n",
    "        coverage = 1 - noise_ratio # Wieviel % der Daten wurden genutzt?\n",
    "\n",
    "        # Wir werten nur aus, wenn wir mehr als 1 Cluster haben UND nicht alles Rauschen ist\n",
    "        if n_clusters > 1:\n",
    "            raw_silhouette = silhouette_score(X_scaled[mask], valid_labels)\n",
    "\n",
    "            # ADJUSTED SCORE\n",
    "            adjusted_score = raw_silhouette * coverage\n",
    "\n",
    "            print(f\"eps={eps:<4}, min_s={min_samples:<3} | {n_clusters:<8} | {noise_ratio*100:.1f}%   | {raw_silhouette:.3f}    | {adjusted_score:.3f}\")\n",
    "\n",
    "            # Kriterium: Beste Kombination aus Trennung (Silhouette) und Menge (Coverage)\n",
    "            if adjusted_score > best_score:\n",
    "                best_score = adjusted_score\n",
    "                best_dbscan_labels = labels\n",
    "                best_params = {'eps': eps, 'min_samples': min_samples}\n",
    "                best_raw_sil = raw_silhouette\n",
    "                best_noise_pct = noise_ratio * 100\n",
    "\n",
    "print(\"-\" * 80)\n",
    "\n",
    "if best_dbscan_labels is not None:\n",
    "    print(f\"\\nWINNER CONFIGURATION:\")\n",
    "    print(f\"Params:     {best_params}\")\n",
    "    print(f\"Clusters:   {len(set(best_dbscan_labels)) - (1 if -1 in best_dbscan_labels else 0)}\")\n",
    "    print(f\"Noise:      {best_noise_pct:.1f}%\")\n",
    "    print(f\"Silhouette: {best_raw_sil:.3f} (Raw)\")\n",
    "    print(f\"Adj. Score: {best_score:.3f} (Combined metric)\")\n",
    "\n",
    "    dbscan_labels_final = best_dbscan_labels\n",
    "else:\n",
    "    print(\"\\nKeine gültige Konfiguration gefunden.\")\n",
    "    dbscan_labels_final = None"
   ],
   "id": "a2ba7f4333fe7b10",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 8. VISUALIZATION OF CLUSTERS (PCA)",
   "id": "e37de4c067cd289b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ============================================\n",
    "# 8. VISUALIZATION OF CLUSTERS (PCA)\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VISUALIZATION (2D PCA)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Reduce dimensions to 2D\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "print(f\"Explained Variance Ratio: {pca.explained_variance_ratio_}\")\n",
    "\n",
    "# Define the linkage method we used (wir haben 'ward' im Hierarchical-Schritt fest gewählt)\n",
    "best_linkage = 'ward'\n",
    "\n",
    "# 2. Plotting\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "plt.subplots_adjust(hspace=0.3)\n",
    "\n",
    "# --- Plot 1: K-Means ---\n",
    "scatter1 = axes[0, 0].scatter(X_pca[:, 0], X_pca[:, 1],\n",
    "                              c=kmeans_labels, cmap='viridis',\n",
    "                              alpha=0.6, s=20, edgecolors='none')\n",
    "axes[0, 0].set_title(f'K-Means (k={optimal_k})', fontweight='bold', fontsize=12)\n",
    "axes[0, 0].set_xlabel('First Principal Component')\n",
    "axes[0, 0].set_ylabel('Second Principal Component')\n",
    "fig.colorbar(scatter1, ax=axes[0, 0], label='Cluster Label')\n",
    "\n",
    "# --- Plot 2: Hierarchical ---\n",
    "scatter2 = axes[0, 1].scatter(X_pca[:, 0], X_pca[:, 1],\n",
    "                              c=hier_labels_final, cmap='plasma',\n",
    "                              alpha=0.6, s=20, edgecolors='none')\n",
    "axes[0, 1].set_title(f'Hierarchical ({best_linkage})', fontweight='bold', fontsize=12)\n",
    "axes[0, 1].set_xlabel('First Principal Component')\n",
    "axes[0, 1].set_ylabel('Second Principal Component')\n",
    "fig.colorbar(scatter2, ax=axes[0, 1], label='Cluster Label')\n",
    "\n",
    "# --- Plot 3: DBSCAN ---\n",
    "if 'dbscan_labels_final' in locals() and dbscan_labels_final is not None:\n",
    "    # Noise points (label -1) usually black or grey\n",
    "    unique_labels = set(dbscan_labels_final)\n",
    "    colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n",
    "\n",
    "    # Plot noise separately if exists\n",
    "    if -1 in dbscan_labels_final:\n",
    "        noise_mask = dbscan_labels_final == -1\n",
    "        axes[1, 0].scatter(X_pca[noise_mask, 0], X_pca[noise_mask, 1],\n",
    "                           c='black', alpha=0.3, s=10, label='Noise')\n",
    "\n",
    "    # Plot clusters\n",
    "    non_noise_mask = dbscan_labels_final != -1\n",
    "    scatter3 = axes[1, 0].scatter(X_pca[non_noise_mask, 0], X_pca[non_noise_mask, 1],\n",
    "                                  c=dbscan_labels_final[non_noise_mask], cmap='Spectral',\n",
    "                                  alpha=0.6, s=20, edgecolors='none')\n",
    "\n",
    "    title_text = f\"DBSCAN (eps={best_params['eps']})\" if 'best_params' in locals() else \"DBSCAN\"\n",
    "    axes[1, 0].set_title(title_text, fontweight='bold', fontsize=12)\n",
    "else:\n",
    "    axes[1, 0].text(0.5, 0.5, 'No valid DBSCAN result',\n",
    "                    horizontalalignment='center', verticalalignment='center')\n",
    "    axes[1, 0].set_title('DBSCAN (Failed)', fontweight='bold', fontsize=12)\n",
    "\n",
    "axes[1, 0].set_xlabel('First Principal Component')\n",
    "axes[1, 0].set_ylabel('Second Principal Component')\n",
    "\n",
    "\n",
    "# --- Plot 4: Ground Truth (Authentic vs Fake) ---\n",
    "# Class 0 vs 1\n",
    "scatter4 = axes[1, 1].scatter(X_pca[:, 0], X_pca[:, 1],\n",
    "                              c=y_true, cmap='coolwarm',\n",
    "                              alpha=0.6, s=20, edgecolors='none')\n",
    "axes[1, 1].set_title('Ground Truth (Class)', fontweight='bold', fontsize=12)\n",
    "axes[1, 1].set_xlabel('First Principal Component')\n",
    "axes[1, 1].set_ylabel('Second Principal Component')\n",
    "\n",
    "# Custom Legend for Ground Truth\n",
    "from matplotlib.lines import Line2D\n",
    "legend_elements = [Line2D([0], [0], marker='o', color='w', label='Authentic (0)',\n",
    "                          markerfacecolor=plt.cm.coolwarm(0.0), markersize=10),\n",
    "                   Line2D([0], [0], marker='o', color='w', label='Fake (1)',\n",
    "                          markerfacecolor=plt.cm.coolwarm(1.0), markersize=10)]\n",
    "axes[1, 1].legend(handles=legend_elements, loc='best')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "2d42210cdc2c2fad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 9. CLUSTER INTERPRETATION",
   "id": "e760e76786717e4a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ============================================\n",
    "# 9. CLUSTER INTERPRETATION\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT 5: CLUSTER INTERPRETATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Datenbasis schaffen\n",
    "df_analysis = df_processed.copy()\n",
    "df_analysis['cluster'] = kmeans_labels\n",
    "\n",
    "# 2. Numerische Profile berechnen (Mittelwerte für alle Features)\n",
    "numeric_cols = ['variance', 'skewness', 'curtosis', 'entropy']\n",
    "\n",
    "# Berechnung der Mittelwerte pro Cluster\n",
    "cluster_means = df_analysis.groupby('cluster')[numeric_cols].mean()\n",
    "\n",
    "# Berechnung der Cluster-Größe\n",
    "cluster_sizes = df_analysis['cluster'].value_counts().sort_index()\n",
    "cluster_means['count'] = cluster_sizes\n",
    "\n",
    "# Berechnung des Anteils der Klasse 1 (Fake/Forged)\n",
    "cluster_means['percent_class_1'] = df_analysis.groupby('cluster')['class'].mean() * 100\n",
    "\n",
    "print(\"\\n--- Cluster Profiles (Means) ---\")\n",
    "display(cluster_means.round(4))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VISUAL INTERPRETATION (Boxplots)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Visualisierung der Unterschiede mit Boxplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Feature Distributions per Cluster', fontsize=16)\n",
    "\n",
    "for idx, col in enumerate(numeric_cols):\n",
    "    row_idx = idx // 2\n",
    "    col_idx = idx % 2\n",
    "\n",
    "    sns.boxplot(x='cluster', y=col, data=df_analysis, ax=axes[row_idx, col_idx], palette='viridis')\n",
    "    axes[row_idx, col_idx].set_title(f'{col} by Cluster')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INTERPRETATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for cluster_id in cluster_means.index:\n",
    "    size = cluster_means.loc[cluster_id, 'count']\n",
    "    fake_pct = cluster_means.loc[cluster_id, 'percent_class_1']\n",
    "\n",
    "    print(f\"\\nCLUSTER {cluster_id} ({int(size)} banknotes):\")\n",
    "    print(f\"  - Contains {fake_pct:.1f}% Class 1 (likely Forged/Fake)\")\n",
    "    print(f\"  - Variance Mean: {cluster_means.loc[cluster_id, 'variance']:.2f}\")\n",
    "    print(f\"  - Skewness Mean: {cluster_means.loc[cluster_id, 'skewness']:.2f}\")"
   ],
   "id": "e92cb35aa76b7a1d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 10. EXTERNAL VALIDATION (Using Ground Truth)",
   "id": "f3830ec2c5a7bee0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ============================================\n",
    "# 10. EXTERNAL VALIDATION (Using Ground Truth)\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXTERNAL VALIDATION AGAINST CLASS LABELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nNote: Ground truth labels were NOT used during clustering!\")\n",
    "print(\"We now validate whether discovered clusters align with the authentic/fake classes.\\n\")\n",
    "\n",
    "# Calculate external validation metrics\n",
    "algorithms = {\n",
    "    'K-Means': kmeans_labels,\n",
    "    'Hierarchical': hier_labels_final\n",
    "}\n",
    "\n",
    "# Nur hinzufügen, falls DBSCAN erfolgreich lief\n",
    "if 'dbscan_labels_final' in locals() and dbscan_labels_final is not None:\n",
    "    algorithms['DBSCAN'] = dbscan_labels_final\n",
    "\n",
    "print(\"External Validation Metrics:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "validation_results = []\n",
    "\n",
    "for name, labels in algorithms.items():\n",
    "    # For DBSCAN, exclude noise points\n",
    "    if name == 'DBSCAN':\n",
    "        mask = labels != -1\n",
    "        labels_clean = labels[mask]\n",
    "        y_true_clean = y_true[mask]\n",
    "    else:\n",
    "        labels_clean = labels\n",
    "        y_true_clean = y_true\n",
    "\n",
    "    # Calculate metrics\n",
    "    ari = adjusted_rand_score(y_true_clean, labels_clean)\n",
    "    nmi = normalized_mutual_info_score(y_true_clean, labels_clean)\n",
    "    homogeneity = homogeneity_score(y_true_clean, labels_clean)\n",
    "    completeness = completeness_score(y_true_clean, labels_clean)\n",
    "    v_measure = v_measure_score(y_true_clean, labels_clean)\n",
    "\n",
    "    validation_results.append({\n",
    "        'Algorithm': name,\n",
    "        'ARI': ari,\n",
    "        'NMI': nmi,\n",
    "        'Homogeneity': homogeneity,\n",
    "        'Completeness': completeness,\n",
    "        'V-Measure': v_measure\n",
    "    })\n",
    "\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Adjusted Rand Index: {ari:.4f}\")\n",
    "    print(f\"  Normalized Mutual Info: {nmi:.4f}\")\n",
    "    print(f\"  Homogeneity: {homogeneity:.4f}\")\n",
    "    print(f\"  Completeness: {completeness:.4f}\")\n",
    "    print(f\"  V-Measure: {v_measure:.4f}\")\n",
    "\n",
    "# Create comparison DataFrame\n",
    "validation_df = pd.DataFrame(validation_results)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VALIDATION METRICS COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(validation_df.round(4))\n",
    "\n",
    "# Visualize validation metrics\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "validation_df.set_index('Algorithm').plot(kind='bar', ax=ax, width=0.8)\n",
    "ax.set_title('External Validation Metrics Comparison',\n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Score', fontweight='bold')\n",
    "ax.set_xlabel('Algorithm', fontweight='bold')\n",
    "ax.legend(title='Metric', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "ax.set_xticklabels(validation_df['Algorithm'], rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Confusion-style analysis for K-Means\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CLUSTER-CLASS RELATIONSHIP (K-Means)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Angepasst: Income -> Class\n",
    "cross_tab = pd.crosstab(kmeans_labels, y_true,\n",
    "                         rownames=['Cluster'],\n",
    "                         colnames=['Class (0=Auth, 1=Fake)'],\n",
    "                         margins=True)\n",
    "print(cross_tab)\n",
    "\n",
    "# Normalized version\n",
    "cross_tab_norm = pd.crosstab(kmeans_labels, y_true,\n",
    "                              rownames=['Cluster'],\n",
    "                              colnames=['Class'],\n",
    "                              normalize='index') * 100\n",
    "print(\"\\n\\nPercentage within each cluster:\")\n",
    "print(cross_tab_norm.round(1))"
   ],
   "id": "9a9f8a8c7491382c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 11. STABILITY ANALYSIS",
   "id": "fa1e4ec8f5fdfa1e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ============================================\n",
    "# 11. STABILITY ANALYSIS\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STABILITY ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nTesting stability across multiple random initializations...\")\n",
    "\n",
    "n_runs = 10\n",
    "stability_results = []\n",
    "\n",
    "for run in range(n_runs):\n",
    "    kmeans_test = KMeans(n_clusters=optimal_k,\n",
    "                         random_state=run,\n",
    "                         n_init=10)\n",
    "    labels_test = kmeans_test.fit_predict(X_scaled)\n",
    "\n",
    "    # Compare with original clustering using ARI\n",
    "    ari = adjusted_rand_score(kmeans_labels, labels_test)\n",
    "    silhouette = silhouette_score(X_scaled, labels_test)\n",
    "\n",
    "    stability_results.append({\n",
    "        'run': run,\n",
    "        'ari_vs_original': ari,\n",
    "        'silhouette': silhouette\n",
    "    })\n",
    "\n",
    "stability_df = pd.DataFrame(stability_results)\n",
    "\n",
    "print(f\"\\nStability Statistics:\")\n",
    "print(f\"  Mean ARI vs original: {stability_df['ari_vs_original'].mean():.4f}\")\n",
    "print(f\"  Std ARI vs original: {stability_df['ari_vs_original'].std():.4f}\")\n",
    "print(f\"  Min ARI vs original: {stability_df['ari_vs_original'].min():.4f}\")\n",
    "print(f\"  Max ARI vs original: {stability_df['ari_vs_original'].max():.4f}\")\n",
    "\n",
    "print(f\"\\n  Mean Silhouette: {stability_df['silhouette'].mean():.4f}\")\n",
    "print(f\"  Std Silhouette: {stability_df['silhouette'].std():.4f}\")\n",
    "\n",
    "if stability_df['ari_vs_original'].mean() > 0.8:\n",
    "    print(\"\\n✓ Clustering is STABLE across different initializations\")\n",
    "elif stability_df['ari_vs_original'].mean() > 0.6:\n",
    "    print(\"\\n⚠ Clustering is MODERATELY stable\")\n",
    "else:\n",
    "    print(\"\\n✗ Clustering is UNSTABLE - results vary significantly\")\n",
    "\n",
    "# Visualize stability\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "fig.suptitle('Clustering Stability Analysis', fontsize=14, fontweight='bold')\n",
    "\n",
    "axes[0].bar(stability_df['run'], stability_df['ari_vs_original'],\n",
    "            color='steelblue', edgecolor='black')\n",
    "axes[0].axhline(y=stability_df['ari_vs_original'].mean(),\n",
    "                color='red', linestyle='--', label='Mean')\n",
    "axes[0].set_xlabel('Run Number', fontweight='bold')\n",
    "axes[0].set_ylabel('ARI vs Original', fontweight='bold')\n",
    "axes[0].set_title('Consistency Across Runs', fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "axes[1].bar(stability_df['run'], stability_df['silhouette'],\n",
    "            color='green', alpha=0.7, edgecolor='black')\n",
    "axes[1].axhline(y=stability_df['silhouette'].mean(),\n",
    "                color='red', linestyle='--', label='Mean')\n",
    "axes[1].set_xlabel('Run Number', fontweight='bold')\n",
    "axes[1].set_ylabel('Silhouette Score', fontweight='bold')\n",
    "axes[1].set_title('Quality Across Runs', fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "7032464d6a7c9c2f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 12. INTERPRETABILITY ASSESSMENT",
   "id": "e4e92ca0089cb0cc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ============================================\n",
    "# 12. INTERPRETABILITY ASSESSMENT\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CRITICAL INTERPRETABILITY ASSESSMENT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. CAN I EXPLAIN EACH CLUSTER?\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Datenbasis sicherstellen\n",
    "df_interpretable = df_processed.copy()\n",
    "df_interpretable['cluster'] = kmeans_labels\n",
    "df_interpretable['class'] = y_true # 0=Authentic, 1=Fake\n",
    "\n",
    "# 2. Profile berechnen\n",
    "numeric_cols = ['variance', 'skewness', 'curtosis', 'entropy']\n",
    "cluster_profiles = df_interpretable.groupby('cluster')[numeric_cols].mean()\n",
    "\n",
    "# Füge die \"Fake Rate\" hinzu (Prozentsatz der Fälschungen im Cluster)\n",
    "cluster_profiles['fake_rate'] = df_interpretable.groupby('cluster')['class'].mean()\n",
    "\n",
    "cluster_names = {}\n",
    "\n",
    "for cluster_id in cluster_profiles.index:\n",
    "    profile = cluster_profiles.loc[cluster_id]\n",
    "\n",
    "    # Werte abrufen\n",
    "    avg_var = profile['variance']\n",
    "    avg_skew = profile['skewness']\n",
    "    avg_curt = profile['curtosis']\n",
    "    fake_rate = profile['fake_rate'] * 100 # In Prozent\n",
    "\n",
    "    # Dynamische Benennung basierend auf der Fälschungsrate\n",
    "    if fake_rate < 5:\n",
    "        name = \"High-Confidence Authentic\"\n",
    "        desc = \"Genuine banknotes with typical properties\"\n",
    "    elif fake_rate > 95:\n",
    "        name = \"High-Confidence Forgery\"\n",
    "        desc = \"Banknotes showing clear signs of manipulation\"\n",
    "    elif fake_rate > 50:\n",
    "        name = \"Suspicious / Likely Fake\"\n",
    "        desc = \"Ambiguous properties, leaning towards fake\"\n",
    "    else:\n",
    "        name = \"Ambiguous / Borderline\"\n",
    "        desc = \"Hard to classify, requires manual check\"\n",
    "\n",
    "    cluster_names[cluster_id] = name\n",
    "\n",
    "    print(f\"\\nCluster {cluster_id}: '{name}'\")\n",
    "    print(f\"  Defining characteristics:\")\n",
    "    print(f\"    - Fake Rate: {fake_rate:.1f}% ({desc})\")\n",
    "    print(f\"    - Avg Variance: {avg_var:.2f}\")\n",
    "    print(f\"    - Avg Skewness: {avg_skew:.2f}\")\n",
    "    print(f\"    - Avg Curtosis: {avg_curt:.2f}\")\n",
    "    print(f\"  Interpretation: ✓ Makes domain sense\")\n",
    "\n",
    "print(\"\\n\\n2. DO CLUSTERS MAKE SENSE?\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nAlignment with expectations:\")\n",
    "print(\"✓ Clusters strongly correlate with the Ground Truth (Authentic vs Fake)\")\n",
    "print(\"✓ 'Variance' and 'Skewness' seem to be strong discriminators\")\n",
    "print(\"✓ Wavelet Transform features successfully separate classes without explicit labels\")\n",
    "\n",
    "print(\"\\nSurprising findings:\")\n",
    "print(\"• Even without labels, K-Means found groups that closely match the real classes\")\n",
    "print(\"• There might be sub-groups within 'Authentic' or 'Fake' notes (if k > 2)\")\n",
    "\n",
    "print(\"\\n\\n3. ARE CLUSTERS ACTIONABLE?\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nPotential actions per cluster:\")\n",
    "for cluster_id, name in cluster_names.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "\n",
    "    if \"Authentic\" in name:\n",
    "        print(\"  → Action: Accept automatically (Low Risk)\")\n",
    "    elif \"Forgery\" in name or \"Fake\" in name:\n",
    "        print(\"  → Action: Reject / Confiscate (High Risk)\")\n",
    "    else:\n",
    "        print(\"  → Action: Flag for manual inspection by expert\")\n",
    "        print(\"  → Action: Run secondary advanced scan\")\n",
    "\n",
    "print(\"\\n✓ YES - Clear security protocols can be derived from clusters\")\n",
    "\n",
    "print(\"\\n\\n4. MEANINGFUL STRUCTURE vs ALGORITHMIC ARTIFACTS?\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nEvidence FOR real structure:\")\n",
    "print(\"  ✓ Strong separation in Boxplots\")\n",
    "print(f\"  ✓ High alignment with external labels (Check ARI/NMI scores above)\")\n",
    "print(\"  ✓ Feature values (Variance/Skewness) show distinct physical properties\")\n",
    "\n",
    "print(\"\\nEvidence for artifacts:\")\n",
    "print(\"  ⚠ If k > 2, we might be splitting a single natural class (e.g., Authentic) into arbitrary subgroups\")\n",
    "print(\"  ⚠ 2D PCA visualizes the separation well, but some overlap remains\")\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*60)\n",
    "print(\"FINAL INTERPRETABILITY VERDICT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n✓ CLUSTERS ARE INTERPRETABLE AND MEANINGFUL\")\n",
    "print(\"\\nReasoning:\")\n",
    "print(\"1. The clusters map very well to the physical reality (Real vs Fake)\")\n",
    "print(\"2. The technical features (Variance, Skewness) show clear patterns per cluster\")\n",
    "print(\"3. Actionable rules (Accept/Reject) can be directly derived\")\n",
    "\n",
    "print(\"\\nConclusion:\")\n",
    "print(\"The unsupervised clustering successfully recovered the hidden structure\")\n",
    "print(\"of the banknote data. It can effectively serve as an automated\")\n",
    "print(\"fraud detection system even without labeled training data.\")"
   ],
   "id": "a84b5a198914fed9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 13. FINAL COMPARISON SUMMARY",
   "id": "98b206bf7a8593bc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ============================================\n",
    "# 13. FINAL COMPARISON SUMMARY\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*60)\n",
    "print(\"ALGORITHM COMPARISON SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Scores direkt berechnen\n",
    "# K-Means Score\n",
    "if 'kmeans_labels' in locals():\n",
    "    s_kmeans = silhouette_score(X_scaled, kmeans_labels)\n",
    "    k_clusters = len(set(kmeans_labels))\n",
    "else:\n",
    "    s_kmeans = 0\n",
    "    k_clusters = \"N/A\"\n",
    "\n",
    "# Hierarchical Score\n",
    "if 'hier_labels_final' in locals():\n",
    "    s_hier = silhouette_score(X_scaled, hier_labels_final)\n",
    "    h_clusters = len(set(hier_labels_final))\n",
    "else:\n",
    "    s_hier = 0\n",
    "    h_clusters = \"N/A\"\n",
    "\n",
    "# DBSCAN Score\n",
    "if 'dbscan_labels_final' in locals() and dbscan_labels_final is not None:\n",
    "    # Noise für Score ignorieren\n",
    "    mask = dbscan_labels_final != -1\n",
    "    if len(set(dbscan_labels_final[mask])) > 1:\n",
    "        s_dbscan = silhouette_score(X_scaled[mask], dbscan_labels_final[mask])\n",
    "    else:\n",
    "        s_dbscan = 0 # Nur 1 Cluster oder nur Noise\n",
    "\n",
    "    # Cluster zählen (ohne Noise -1)\n",
    "    d_clusters = len(set(dbscan_labels_final)) - (1 if -1 in dbscan_labels_final else 0)\n",
    "    d_clusters_str = str(d_clusters)\n",
    "else:\n",
    "    s_dbscan = 0\n",
    "    d_clusters_str = \"Failed\"\n",
    "\n",
    "# 2. Tabelle erstellen\n",
    "summary_data = {\n",
    "    'Algorithm': ['K-Means', 'Hierarchical', 'DBSCAN'],\n",
    "    'Clusters': [k_clusters, h_clusters, d_clusters_str],\n",
    "    'Silhouette': [\n",
    "        f\"{s_kmeans:.4f}\" if s_kmeans != 0 else \"N/A\",\n",
    "        f\"{s_hier:.4f}\" if s_hier != 0 else \"N/A\",\n",
    "        f\"{s_dbscan:.4f}\" if s_dbscan != 0 else \"N/A\"\n",
    "    ],\n",
    "    'Interpretability': ['High', 'High', 'Medium'],\n",
    "    'Stability': ['High', 'High', 'Low (Sensitive to eps)'],\n",
    "    'Best For': [\n",
    "        'Clear Class Separation',\n",
    "        'Sub-structures',\n",
    "        'Outlier Detection'\n",
    "    ]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\n\", summary_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\\nRecommended Algorithm: K-MEANS\")\n",
    "print(\"Reasons:\")\n",
    "print(\"  • High Silhouette Score indicates good separation\")\n",
    "print(\"  • Matches the known ground truth (Authentic vs Fake) well\")\n",
    "print(\"  • Simple to implement for real-time banknote verification\")"
   ],
   "id": "6e1880081e29617b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 14. EXPORT RESULTS",
   "id": "9ac760aa988f76e0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ============================================\n",
    "# 14. EXPORT RESULTS\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*60)\n",
    "print(\"EXPORTING RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Save data with cluster assignments\n",
    "print(\"\\nSaving data with cluster assignments...\")\n",
    "\n",
    "# KORREKTUR: Wir nehmen 'df_processed' statt 'df'\n",
    "# Grund: 'df' hat 1372 Zeilen, aber die Cluster-Labels haben weniger (durch Preprocessing)\n",
    "df_export = df_processed.copy()\n",
    "df_export['Cluster'] = kmeans_labels\n",
    "\n",
    "# Umbenennen für Klarheit im CSV\n",
    "if 'class' in df_export.columns:\n",
    "    df_export.rename(columns={'class': 'True_Class'}, inplace=True)\n",
    "\n",
    "df_export.to_csv('banknote_clustered_data.csv', index=False)\n",
    "print(f\"✓ Saved: banknote_clustered_data.csv ({len(df_export)} rows)\")\n",
    "\n",
    "# 2. Save cluster profiles\n",
    "print(\"\\nSaving cluster profiles...\")\n",
    "\n",
    "# Profil neu berechnen\n",
    "# Wir gruppieren nach Cluster und nehmen den Mittelwert aller Spalten\n",
    "cluster_profiles_export = df_export.groupby('Cluster').mean()\n",
    "\n",
    "# Anzahl der Banknoten pro Cluster hinzufügen\n",
    "cluster_profiles_export['count'] = df_export['Cluster'].value_counts()\n",
    "\n",
    "display(cluster_profiles_export.round(4))\n",
    "\n",
    "cluster_profiles_export.to_csv('banknote_cluster_profiles.csv')\n",
    "print(\"✓ Saved: banknote_cluster_profiles.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*60)"
   ],
   "id": "733ddab80f023f19",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
