{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Paper 1: Regularization Effectiveness Study\n",
    "## Autoren: Katrin Nguyen & Nico Loss"
   ],
   "id": "83ab7e8787bcb399"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import & Setup",
   "id": "3a6e7c0cae11a712"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import kagglehub\n",
    "import warnings\n",
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.linear_model import RidgeCV, LassoCV, ElasticNetCV\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, learning_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "# Warnungen unterdrücken\n",
    "warnings.filterwarnings('ignore', category=ConvergenceWarning)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Plot settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "Path(\"Reports\").mkdir(exist_ok=True)\n",
    "Path(\"Save\").mkdir(exist_ok=True)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data Loading\n",
    "Download Dataset from Kaggle.com: https://www.kaggle.com/datasets/shivam2503/diamonds"
   ],
   "id": "30ea02af119bff5d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "path = kagglehub.dataset_download(\"shivam2503/diamonds\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "df = pd.read_csv(path + \"\\\\diamonds.csv\")\n",
    "\n",
    "df.drop(columns=[\"Unnamed: 0\"], inplace=True) # Fehlerhafte Index Column entfernen"
   ],
   "id": "f76c3d3c3eceb2b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Exploration",
   "id": "ea3a6270ac4095ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"\\nDataset Info:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nStatistical Summary:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum())"
   ],
   "id": "7e2de27169535001",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Automatisches Reporting erstellen",
   "id": "24fd5b90b0ff532c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "profile = ProfileReport(df, title=\"Analyse Diamond Report\", explorative=True)\n",
    "profile.to_file(\"Reports\\\\DiamonAnalyses_ydata_profiling.html\")"
   ],
   "id": "c8cbbb75f3c9b97",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Explorative Data Analysis",
   "id": "dec6212481afbf2f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EXPLORATORY DATA ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create figure for EDA plots\n",
    "fig = plt.figure(figsize=(15, 12))\n",
    "\n",
    "# 1. Price distribution\n",
    "ax1 = plt.subplot(3, 3, 1)\n",
    "df['price'].hist(bins=50, edgecolor='black')\n",
    "plt.title('Price Distribution')\n",
    "plt.xlabel('Price ($)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# 2. Log-transformed price distribution\n",
    "ax2 = plt.subplot(3, 3, 2)\n",
    "np.log1p(df['price']).hist(bins=50, edgecolor='black')\n",
    "plt.title('Log(Price) Distribution')\n",
    "plt.xlabel('Log(Price)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# 3. Price vs Carat\n",
    "ax3 = plt.subplot(3, 3, 3)\n",
    "plt.scatter(df['carat'], df['price'], alpha=0.3, s=1)\n",
    "plt.title('Price vs Carat')\n",
    "plt.xlabel('Carat')\n",
    "plt.ylabel('Price ($)')\n",
    "\n",
    "# 4. Price by Cut\n",
    "ax4 = plt.subplot(3, 3, 4)\n",
    "df.boxplot(column='price', by='cut', ax=ax4)\n",
    "plt.title('Price Distribution by Cut')\n",
    "plt.suptitle('')\n",
    "plt.xlabel('Cut Quality')\n",
    "plt.ylabel('Price ($)')\n",
    "\n",
    "# 5. Price by Color\n",
    "ax5 = plt.subplot(3, 3, 5)\n",
    "df.boxplot(column='price', by='color', ax=ax5)\n",
    "plt.title('Price Distribution by Color')\n",
    "plt.suptitle('')\n",
    "plt.xlabel('Color Grade')\n",
    "plt.ylabel('Price ($)')\n",
    "\n",
    "# 6. Price by Clarity\n",
    "ax6 = plt.subplot(3, 3, 6)\n",
    "df.boxplot(column='price', by='clarity', ax=ax6)\n",
    "plt.title('Price Distribution by Clarity')\n",
    "plt.suptitle('')\n",
    "plt.xlabel('Clarity Grade')\n",
    "plt.ylabel('Price ($)')\n",
    "\n",
    "# 7. Carat distribution\n",
    "ax7 = plt.subplot(3, 3, 7)\n",
    "df['carat'].hist(bins=50, edgecolor='black')\n",
    "plt.title('Carat Distribution')\n",
    "plt.xlabel('Carat')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# 8. Correlation with price\n",
    "ax8 = plt.subplot(3, 3, 8)\n",
    "numeric_cols = ['carat', 'depth', 'table', 'x', 'y', 'z']\n",
    "correlations = df[numeric_cols + ['price']].corr()['price'].drop('price').sort_values()\n",
    "correlations.plot(kind='barh')\n",
    "plt.title('Feature Correlations with Price')\n",
    "plt.xlabel('Correlation Coefficient')\n",
    "\n",
    "# 9. Feature distributions\n",
    "ax9 = plt.subplot(3, 3, 9)\n",
    "df[['depth', 'table']].boxplot(ax=ax9)\n",
    "plt.title('Depth and Table Distributions')\n",
    "plt.ylabel('Percentage')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Save\\\\eda_diamonds.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "id": "2817ef47e96d2631",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Preprocessing",
   "id": "a4a58dcdd92894aa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DATA PREPROCESSING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Remove diamonds with zero dimensions\n",
    "print(f\"Initial dataset size: {len(df)}\")\n",
    "df_clean = df[(df['x'] > 0) & (df['y'] > 0) & (df['z'] > 0)].copy()\n",
    "print(f\"After removing zero dimensions: {len(df_clean)}\")\n",
    "\n",
    "# Remove extreme outliers (unrealistic measurements)\n",
    "# Using IQR method for dimensions\n",
    "for col in ['x', 'y', 'z']:\n",
    "    Q1 = df_clean[col].quantile(0.25)\n",
    "    Q3 = df_clean[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 3 * IQR\n",
    "    upper_bound = Q3 + 3 * IQR\n",
    "    df_clean = df_clean[(df_clean[col] >= lower_bound) & (df_clean[col] <= upper_bound)]\n",
    "\n",
    "print(f\"After removing outliers: {len(df_clean)}\")\n",
    "\n",
    "# Ordinal encoding for categorical variables\n",
    "cut_mapping = {'Fair': 1, 'Good': 2, 'Very Good': 3, 'Premium': 4, 'Ideal': 5}\n",
    "color_mapping = {'J': 1, 'I': 2, 'H': 3, 'G': 4, 'F': 5, 'E': 6, 'D': 7}\n",
    "clarity_mapping = {'I1': 1, 'SI2': 2, 'SI1': 3, 'VS2': 4, 'VS1': 5, 'VVS2': 6, 'VVS1': 7, 'IF': 8}\n",
    "\n",
    "df_clean['cut_encoded'] = df_clean['cut'].map(cut_mapping)\n",
    "df_clean['color_encoded'] = df_clean['color'].map(color_mapping)\n",
    "df_clean['clarity_encoded'] = df_clean['clarity'].map(clarity_mapping)\n",
    "\n",
    "# Feature engineering\n",
    "print(\"\\nFeature Engineering:\")\n",
    "df_clean['volume'] = df_clean['x'] * df_clean['y'] * df_clean['z']\n",
    "df_clean['carat_squared'] = df_clean['carat'] ** 2\n",
    "df_clean['xy_ratio'] = df_clean['x'] / df_clean['y']\n",
    "df_clean['depth_table_ratio'] = df_clean['depth'] / df_clean['table']\n",
    "print(\"- Added: volume, carat_squared, xy_ratio, depth_table_ratio\")\n",
    "\n",
    "# Prepare features and target\n",
    "feature_cols = ['carat', 'cut_encoded', 'color_encoded', 'clarity_encoded',\n",
    "                'depth', 'table', 'x', 'y', 'z', 'volume', 'carat_squared',\n",
    "                'xy_ratio', 'depth_table_ratio']\n",
    "\n",
    "X = df_clean[feature_cols].values\n",
    "y = df_clean['price'].values\n",
    "\n",
    "print(f\"\\nFinal dataset shape: X={X.shape}, y={y.shape}\")\n",
    "print(f\"Features used: {feature_cols}\")\n",
    "\n",
    "# Check for multicollinearity\n",
    "correlation_matrix = df_clean[feature_cols].corr()\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "            fmt='.2f', square=True, linewidths=1)\n",
    "plt.title('Feature Correlation Matrix - Checking Multicollinearity')\n",
    "plt.tight_layout()\n",
    "plt.savefig('Save\\\\correlation_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Identify highly correlated features\n",
    "high_corr_pairs = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.8:\n",
    "            high_corr_pairs.append((correlation_matrix.columns[i],\n",
    "                                   correlation_matrix.columns[j],\n",
    "                                   correlation_matrix.iloc[i, j]))\n",
    "\n",
    "print(\"\\nHighly correlated feature pairs (|r| > 0.8):\")\n",
    "for pair in high_corr_pairs:\n",
    "    print(f\"  {pair[0]} <-> {pair[1]}: {pair[2]:.3f}\")"
   ],
   "id": "17b1c016b7910ffd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train-Validation-Test Split",
   "id": "278f4fe0294767cf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DATA SPLITTING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# First split: separate test set\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Second split: separate train and validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Feature scaling (fit on training data only)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\nFeatures scaled using StandardScaler (fitted on training set only)\")"
   ],
   "id": "49051e11e5b74916",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Baseline-Model",
   "id": "784b5da978e902f5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"BASELINE MODEL - ORDINARY LEAST SQUARES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Train baseline model\n",
    "baseline = LinearRegression()\n",
    "baseline.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_baseline = baseline.predict(X_train_scaled)\n",
    "y_val_pred_baseline = baseline.predict(X_val_scaled)\n",
    "y_test_pred_baseline = baseline.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate baseline\n",
    "baseline_metrics = {\n",
    "    'Train R²': r2_score(y_train, y_train_pred_baseline),\n",
    "    'Val R²': r2_score(y_val, y_val_pred_baseline),\n",
    "    'Test R²': r2_score(y_test, y_test_pred_baseline),\n",
    "    'Train RMSE': np.sqrt(mean_squared_error(y_train, y_train_pred_baseline)),\n",
    "    'Val RMSE': np.sqrt(mean_squared_error(y_val, y_val_pred_baseline)),\n",
    "    'Test RMSE': np.sqrt(mean_squared_error(y_test, y_test_pred_baseline)),\n",
    "    'Train MAE': mean_absolute_error(y_train, y_train_pred_baseline),\n",
    "    'Val MAE': mean_absolute_error(y_val, y_val_pred_baseline),\n",
    "    'Test MAE': mean_absolute_error(y_test, y_test_pred_baseline)\n",
    "}\n",
    "\n",
    "print(\"\\nBaseline Performance:\")\n",
    "for metric, value in baseline_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "train_test_gap = baseline_metrics['Train R²'] - baseline_metrics['Test R²']\n",
    "print(f\"\\nTrain-Test R² Gap: {train_test_gap:.4f}\")\n",
    "if train_test_gap > 0.05:\n",
    "    print(\"  → Indication of overfitting\")"
   ],
   "id": "1ef81529ebf85323",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Ridge Regression",
   "id": "191b68ef17e49169"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"RIDGE REGRESSION (L2 REGULARIZATION)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Hyperparameter tuning with cross-validation\n",
    "alphas_ridge = np.logspace(-4, 4, 50)\n",
    "ridge_cv = RidgeCV(alphas=alphas_ridge, cv=5, scoring='r2')\n",
    "ridge_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Optimal alpha: {ridge_cv.alpha_:.6f}\")\n",
    "\n",
    "# Train final Ridge model\n",
    "ridge_best = Ridge(alpha=ridge_cv.alpha_)\n",
    "ridge_best.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_ridge = ridge_best.predict(X_train_scaled)\n",
    "y_val_pred_ridge = ridge_best.predict(X_val_scaled)\n",
    "y_test_pred_ridge = ridge_best.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate Ridge\n",
    "ridge_metrics = {\n",
    "    'Train R²': r2_score(y_train, y_train_pred_ridge),\n",
    "    'Val R²': r2_score(y_val, y_val_pred_ridge),\n",
    "    'Test R²': r2_score(y_test, y_test_pred_ridge),\n",
    "    'Train RMSE': np.sqrt(mean_squared_error(y_train, y_train_pred_ridge)),\n",
    "    'Val RMSE': np.sqrt(mean_squared_error(y_val, y_val_pred_ridge)),\n",
    "    'Test RMSE': np.sqrt(mean_squared_error(y_test, y_test_pred_ridge)),\n",
    "    'Train MAE': mean_absolute_error(y_train, y_train_pred_ridge),\n",
    "    'Val MAE': mean_absolute_error(y_val, y_val_pred_ridge),\n",
    "    'Test MAE': mean_absolute_error(y_test, y_test_pred_ridge)\n",
    "}\n",
    "\n",
    "print(\"\\nRidge Performance:\")\n",
    "for metric, value in ridge_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")"
   ],
   "id": "1884abd975a6a5b2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Lasso Regression",
   "id": "b17db107e8d5c2d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"LASSO REGRESSION (L1 REGULARIZATION)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Hyperparameter tuning with cross-validation\n",
    "alphas_lasso = np.logspace(-4, 2, 50)\n",
    "lasso_cv = LassoCV(alphas=alphas_lasso, cv=5, max_iter=10000, random_state=42)\n",
    "lasso_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Optimal alpha: {lasso_cv.alpha_:.6f}\")\n",
    "\n",
    "# Train final Lasso model\n",
    "lasso_best = Lasso(alpha=lasso_cv.alpha_, max_iter=10000)\n",
    "lasso_best.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_lasso = lasso_best.predict(X_train_scaled)\n",
    "y_val_pred_lasso = lasso_best.predict(X_val_scaled)\n",
    "y_test_pred_lasso = lasso_best.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate Lasso\n",
    "lasso_metrics = {\n",
    "    'Train R²': r2_score(y_train, y_train_pred_lasso),\n",
    "    'Val R²': r2_score(y_val, y_val_pred_lasso),\n",
    "    'Test R²': r2_score(y_test, y_test_pred_lasso),\n",
    "    'Train RMSE': np.sqrt(mean_squared_error(y_train, y_train_pred_lasso)),\n",
    "    'Val RMSE': np.sqrt(mean_squared_error(y_val, y_val_pred_lasso)),\n",
    "    'Test RMSE': np.sqrt(mean_squared_error(y_test, y_test_pred_lasso)),\n",
    "    'Train MAE': mean_absolute_error(y_train, y_train_pred_lasso),\n",
    "    'Val MAE': mean_absolute_error(y_val, y_val_pred_lasso),\n",
    "    'Test MAE': mean_absolute_error(y_test, y_test_pred_lasso)\n",
    "}\n",
    "\n",
    "print(\"\\nLasso Performance:\")\n",
    "for metric, value in lasso_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "# Feature selection analysis\n",
    "n_features_selected = np.sum(lasso_best.coef_ != 0)\n",
    "print(f\"\\nFeatures selected by Lasso: {n_features_selected}/{len(feature_cols)}\")\n",
    "print(\"Selected features:\")\n",
    "for i, (feature, coef) in enumerate(zip(feature_cols, lasso_best.coef_)):\n",
    "    if coef != 0:\n",
    "        print(f\"  {feature}: {coef:.4f}\")"
   ],
   "id": "69d45c824b64149e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Elastic Net",
   "id": "cf1704adf21087d9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ELASTIC NET (L1 + L2 REGULARIZATION)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Hyperparameter tuning with cross-validation\n",
    "alphas_elastic = np.logspace(-4, 2, 30)\n",
    "l1_ratios = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "elastic_cv = ElasticNetCV(\n",
    "    alphas=alphas_elastic,\n",
    "    l1_ratio=l1_ratios,\n",
    "    cv=5,\n",
    "    max_iter=10000,\n",
    "    random_state=42\n",
    ")\n",
    "elastic_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Optimal alpha: {elastic_cv.alpha_:.6f}\")\n",
    "print(f\"Optimal l1_ratio: {elastic_cv.l1_ratio_:.2f}\")\n",
    "\n",
    "# Train final ElasticNet model\n",
    "elastic_best = ElasticNet(\n",
    "    alpha=elastic_cv.alpha_,\n",
    "    l1_ratio=elastic_cv.l1_ratio_,\n",
    "    max_iter=10000\n",
    ")\n",
    "elastic_best.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_elastic = elastic_best.predict(X_train_scaled)\n",
    "y_val_pred_elastic = elastic_best.predict(X_val_scaled)\n",
    "y_test_pred_elastic = elastic_best.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate ElasticNet\n",
    "elastic_metrics = {\n",
    "    'Train R²': r2_score(y_train, y_train_pred_elastic),\n",
    "    'Val R²': r2_score(y_val, y_val_pred_elastic),\n",
    "    'Test R²': r2_score(y_test, y_test_pred_elastic),\n",
    "    'Train RMSE': np.sqrt(mean_squared_error(y_train, y_train_pred_elastic)),\n",
    "    'Val RMSE': np.sqrt(mean_squared_error(y_val, y_val_pred_elastic)),\n",
    "    'Test RMSE': np.sqrt(mean_squared_error(y_test, y_test_pred_elastic)),\n",
    "    'Train MAE': mean_absolute_error(y_train, y_train_pred_elastic),\n",
    "    'Val MAE': mean_absolute_error(y_val, y_val_pred_elastic),\n",
    "    'Test MAE': mean_absolute_error(y_test, y_test_pred_elastic)\n",
    "}\n",
    "\n",
    "print(\"\\nElasticNet Performance:\")\n",
    "for metric, value in elastic_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")"
   ],
   "id": "8f201f1ba42fb0ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model Comparison",
   "id": "6d5ff78c08c84f58"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_data = {\n",
    "    'OLS': baseline_metrics,\n",
    "    'Ridge': ridge_metrics,\n",
    "    'Lasso': lasso_metrics,\n",
    "    'ElasticNet': elastic_metrics\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data).T\n",
    "print(\"\\nPerformance Comparison Table:\")\n",
    "print(comparison_df.round(4))\n",
    "\n",
    "# Calculate improvements over baseline\n",
    "print(\"\\nImprovement over OLS (Test R²):\")\n",
    "for model in ['Ridge', 'Lasso', 'ElasticNet']:\n",
    "    improvement = comparison_df.loc[model, 'Test R²'] - comparison_df.loc['OLS', 'Test R²']\n",
    "    pct_improvement = (improvement / comparison_df.loc['OLS', 'Test R²']) * 100\n",
    "    print(f\"  {model}: {improvement:.4f} ({pct_improvement:.2f}%)\")\n",
    "\n",
    "# Train-Test Gap Analysis\n",
    "print(\"\\nTrain-Test R² Gap (Overfitting Analysis):\")\n",
    "for model in comparison_df.index:\n",
    "    gap = comparison_df.loc[model, 'Train R²'] - comparison_df.loc[model, 'Test R²']\n",
    "    print(f\"  {model}: {gap:.4f}\")"
   ],
   "id": "63ca8a8aff9c9e75",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Visualizations",
   "id": "6336cd434ce46b59"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"GENERATING VISUALIZATIONS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 1. Model Comparison Bar Plot\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# R² Comparison\n",
    "ax1 = axes[0, 0]\n",
    "models = ['OLS', 'Ridge', 'Lasso', 'ElasticNet']\n",
    "train_r2 = [comparison_df.loc[m, 'Train R²'] for m in models]\n",
    "val_r2 = [comparison_df.loc[m, 'Val R²'] for m in models]\n",
    "test_r2 = [comparison_df.loc[m, 'Test R²'] for m in models]\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.25\n",
    "\n",
    "ax1.bar(x - width, train_r2, width, label='Train', alpha=0.8)\n",
    "ax1.bar(x, val_r2, width, label='Validation', alpha=0.8)\n",
    "ax1.bar(x + width, test_r2, width, label='Test', alpha=0.8)\n",
    "ax1.set_xlabel('Model')\n",
    "ax1.set_ylabel('R² Score')\n",
    "ax1.set_title('Model Performance Comparison - R² Score')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(models)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# RMSE Comparison\n",
    "ax2 = axes[0, 1]\n",
    "train_rmse = [comparison_df.loc[m, 'Train RMSE'] for m in models]\n",
    "val_rmse = [comparison_df.loc[m, 'Val RMSE'] for m in models]\n",
    "test_rmse = [comparison_df.loc[m, 'Test RMSE'] for m in models]\n",
    "\n",
    "ax2.bar(x - width, train_rmse, width, label='Train', alpha=0.8)\n",
    "ax2.bar(x, val_rmse, width, label='Validation', alpha=0.8)\n",
    "ax2.bar(x + width, test_rmse, width, label='Test', alpha=0.8)\n",
    "ax2.set_xlabel('Model')\n",
    "ax2.set_ylabel('RMSE ($)')\n",
    "ax2.set_title('Model Performance Comparison - RMSE')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(models)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Train-Test Gap\n",
    "ax3 = axes[1, 0]\n",
    "gaps = [comparison_df.loc[m, 'Train R²'] - comparison_df.loc[m, 'Test R²'] for m in models]\n",
    "colors = ['red' if g > 0.05 else 'green' for g in gaps]\n",
    "ax3.bar(models, gaps, color=colors, alpha=0.7)\n",
    "ax3.axhline(y=0.05, color='orange', linestyle='--', label='Overfitting Threshold')\n",
    "ax3.set_xlabel('Model')\n",
    "ax3.set_ylabel('Train-Test R² Gap')\n",
    "ax3.set_title('Overfitting Analysis')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Feature Coefficients\n",
    "ax4 = axes[1, 1]\n",
    "coef_data = pd.DataFrame({\n",
    "    'OLS': baseline.coef_,\n",
    "    'Ridge': ridge_best.coef_,\n",
    "    'Lasso': lasso_best.coef_,\n",
    "    'ElasticNet': elastic_best.coef_\n",
    "}, index=feature_cols)\n",
    "\n",
    "# Plot only most important features\n",
    "important_features = coef_data.abs().max(axis=1).nlargest(8).index\n",
    "coef_data.loc[important_features].plot(kind='bar', ax=ax4)\n",
    "ax4.set_xlabel('Feature')\n",
    "ax4.set_ylabel('Coefficient Value')\n",
    "ax4.set_title('Feature Coefficients Comparison (Top 8 Features)')\n",
    "ax4.legend(title='Model')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "plt.setp(ax4.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Save\\\\model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "id": "4031a8fdb23a8dbe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2. Learning Curves\n",
    "print(\"\\nGenerating learning curves...\")\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "models_dict = {\n",
    "    'OLS': LinearRegression(),\n",
    "    'Ridge': Ridge(alpha=ridge_cv.alpha_),\n",
    "    'Lasso': Lasso(alpha=lasso_cv.alpha_, max_iter=10000),\n",
    "    'ElasticNet': ElasticNet(alpha=elastic_cv.alpha_, l1_ratio=elastic_cv.l1_ratio_, max_iter=10000)\n",
    "}\n",
    "\n",
    "train_sizes = np.linspace(0.1, 1.0, 10)\n",
    "\n",
    "for idx, (model_name, model) in enumerate(models_dict.items()):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "\n",
    "    train_sizes_abs, train_scores, val_scores = learning_curve(\n",
    "        model, X_train_scaled, y_train,\n",
    "        train_sizes=train_sizes, cv=5,\n",
    "        scoring='r2', n_jobs=-1, random_state=42\n",
    "    )\n",
    "\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    val_scores_mean = np.mean(val_scores, axis=1)\n",
    "    val_scores_std = np.std(val_scores, axis=1)\n",
    "\n",
    "    ax.plot(train_sizes_abs, train_scores_mean, 'o-', color='blue',\n",
    "            label='Training score', markersize=5)\n",
    "    ax.fill_between(train_sizes_abs, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1, color='blue')\n",
    "\n",
    "    ax.plot(train_sizes_abs, val_scores_mean, 'o-', color='red',\n",
    "            label='Cross-validation score', markersize=5)\n",
    "    ax.fill_between(train_sizes_abs, val_scores_mean - val_scores_std,\n",
    "                     val_scores_mean + val_scores_std, alpha=0.1, color='red')\n",
    "\n",
    "    ax.set_xlabel('Training Set Size')\n",
    "    ax.set_ylabel('R² Score')\n",
    "    ax.set_title(f'Learning Curve - {model_name}')\n",
    "    ax.legend(loc='best')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim([0.8, 1.0])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Save\\\\learning_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "id": "59f14dc8555d6bba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3. Regularization Paths\n",
    "print(\"\\nGenerating regularization paths...\")\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# ===== RIDGE PATH =====\n",
    "print(\"Computing Ridge path...\")\n",
    "alphas_path = np.logspace(-4, 3, 30)\n",
    "ridge_coefs = []\n",
    "\n",
    "for i, alpha in enumerate(alphas_path):\n",
    "    if i % 10 == 0:  # Progress indicator\n",
    "        print(f\"  Ridge: {i}/{len(alphas_path)}\")\n",
    "    ridge_temp = Ridge(alpha=alpha)\n",
    "    ridge_temp.fit(X_train_scaled, y_train)\n",
    "    ridge_coefs.append(ridge_temp.coef_)\n",
    "\n",
    "ridge_coefs = np.array(ridge_coefs)\n",
    "\n",
    "ax1 = axes[0]\n",
    "# Plot nur die 5 wichtigsten Features für Übersichtlichkeit\n",
    "for i, feature in enumerate(feature_cols[:5]):\n",
    "    ax1.plot(alphas_path, ridge_coefs[:, i], label=feature, linewidth=2)\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_xlabel('Alpha (λ)')\n",
    "ax1.set_ylabel('Coefficient Value')\n",
    "ax1.set_title('Ridge Regularization Path')\n",
    "ax1.axvline(x=ridge_cv.alpha_, color='red', linestyle='--', alpha=0.7, label='Optimal α')\n",
    "ax1.legend(loc='best', fontsize='small')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# ===== LASSO PATH (mit Feature Count) =====\n",
    "print(\"Computing Lasso path...\")\n",
    "# Weniger Alphas für schnellere Berechnung\n",
    "alphas_path_lasso = np.logspace(-4, 2, 30)  # Nur 25 statt 80 Punkte\n",
    "lasso_coefs = []\n",
    "n_features_selected = []\n",
    "\n",
    "for i, alpha in enumerate(alphas_path_lasso):\n",
    "    if i % 5 == 0:  # Progress indicator\n",
    "        print(f\"  Lasso: {i}/{len(alphas_path_lasso)}\")\n",
    "\n",
    "    # Angepasste Parameter für bessere Konvergenz\n",
    "    lasso_temp = Lasso(alpha=alpha,\n",
    "                       max_iter=3000,  # Reduziert von 5000\n",
    "                       tol=1e-3,       # Erhöhte Toleranz für schnellere Konvergenz\n",
    "                       warm_start=True)\n",
    "    lasso_temp.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Speichere Koeffizienten UND Feature Count in einem Durchgang\n",
    "    lasso_coefs.append(lasso_temp.coef_)\n",
    "    n_features_selected.append(np.sum(np.abs(lasso_temp.coef_) > 1e-5))\n",
    "\n",
    "lasso_coefs = np.array(lasso_coefs)\n",
    "\n",
    "ax2 = axes[1]\n",
    "for i, feature in enumerate(feature_cols[:5]):\n",
    "    ax2.plot(alphas_path_lasso, lasso_coefs[:, i], label=feature, linewidth=2)\n",
    "ax2.set_xscale('log')\n",
    "ax2.set_xlabel('Alpha (λ)')\n",
    "ax2.set_ylabel('Coefficient Value')\n",
    "ax2.set_title('Lasso Regularization Path')\n",
    "ax2.axvline(x=lasso_cv.alpha_, color='red', linestyle='--', alpha=0.7, label='Optimal α')\n",
    "ax2.legend(loc='best', fontsize='small')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# ===== FEATURE SELECTION COUNT (bereits berechnet!) =====\n",
    "ax3 = axes[2]\n",
    "ax3.plot(alphas_path_lasso, n_features_selected, 'o-', markersize=6, linewidth=2, color='darkblue')\n",
    "ax3.fill_between(alphas_path_lasso, 0, n_features_selected, alpha=0.2, color='blue')\n",
    "ax3.set_xscale('log')\n",
    "ax3.set_xlabel('Alpha (λ)')\n",
    "ax3.set_ylabel('Number of Selected Features')\n",
    "ax3.set_title('Feature Selection vs. Regularization Strength')\n",
    "ax3.axvline(x=lasso_cv.alpha_, color='red', linestyle='--', alpha=0.7, label='Optimal α')\n",
    "ax3.axhline(y=len(feature_cols), color='gray', linestyle=':', alpha=0.5, label='Total Features')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Save\\\\regularization_paths.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Regularization paths completed!\")"
   ],
   "id": "ccbebace6a78fc36",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4. Cross-validation curves (OPTIMIZED)\n",
    "print(\"\\nGenerating cross-validation curves...\")\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# ===== RIDGE CV (schnelle Version) =====\n",
    "print(\"Computing Ridge CV scores...\")\n",
    "ax1 = axes[0]\n",
    "\n",
    "# Weniger Alpha-Werte für schnellere Berechnung\n",
    "alphas_ridge_cv = np.logspace(-4, 2, 30)\n",
    "ridge_scores_mean = []\n",
    "ridge_scores_std = []\n",
    "\n",
    "for i, alpha in enumerate(alphas_ridge_cv):\n",
    "    if i % 5 == 0:\n",
    "        print(f\"  Ridge progress: {i}/{len(alphas_ridge_cv)}\")\n",
    "    ridge_temp = Ridge(alpha=alpha)\n",
    "    scores = cross_val_score(ridge_temp,\n",
    "                             X_train_scaled,\n",
    "                             y_train,\n",
    "                             cv=5, scoring='r2', n_jobs=-1)\n",
    "    ridge_scores_mean.append(scores.mean())\n",
    "    ridge_scores_std.append(scores.std())\n",
    "\n",
    "ridge_scores_mean = np.array(ridge_scores_mean)\n",
    "ridge_scores_std = np.array(ridge_scores_std)\n",
    "\n",
    "ax1.plot(alphas_ridge_cv, ridge_scores_mean, 'b-', linewidth=2, label='Mean CV Score')\n",
    "ax1.fill_between(alphas_ridge_cv,\n",
    "                  ridge_scores_mean - ridge_scores_std,\n",
    "                  ridge_scores_mean + ridge_scores_std,\n",
    "                  alpha=0.2, label='±1 std')\n",
    "ax1.axvline(x=ridge_cv.alpha_, color='red', linestyle='--',\n",
    "            linewidth=2, label=f'Best α={ridge_cv.alpha_:.4f}')\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_xlabel('Alpha (λ)')\n",
    "ax1.set_ylabel('Cross-validation R² Score')\n",
    "ax1.set_title('Ridge Cross-validation Scores')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# ===== LASSO CV (schnelle Version) =====\n",
    "print(\"Computing Lasso CV scores...\")\n",
    "ax2 = axes[1]\n",
    "\n",
    "# Weniger Alpha-Werte und vermeidung problematischer Bereiche\n",
    "alphas_lasso_cv = np.logspace(-4, 2, 30)\n",
    "lasso_scores_mean = []\n",
    "lasso_scores_std = []\n",
    "\n",
    "for i, alpha in enumerate(alphas_lasso_cv):\n",
    "    if i % 5 == 0:\n",
    "        print(f\"  Lasso progress: {i}/{len(alphas_lasso_cv)}\")\n",
    "    lasso_temp = Lasso(alpha=alpha, max_iter=2000, tol=1e-3)\n",
    "    scores = cross_val_score(lasso_temp, X_train_scaled, y_train,\n",
    "                           cv=5, scoring='r2', n_jobs=-1)\n",
    "    lasso_scores_mean.append(scores.mean())\n",
    "    lasso_scores_std.append(scores.std())\n",
    "\n",
    "lasso_scores_mean = np.array(lasso_scores_mean)\n",
    "lasso_scores_std = np.array(lasso_scores_std)\n",
    "\n",
    "ax2.plot(alphas_lasso_cv, lasso_scores_mean, 'g-', linewidth=2, label='Mean CV Score')\n",
    "ax2.fill_between(alphas_lasso_cv,\n",
    "                  lasso_scores_mean - lasso_scores_std,\n",
    "                  lasso_scores_mean + lasso_scores_std,\n",
    "                  alpha=0.2, label='±1 std')\n",
    "ax2.axvline(x=lasso_cv.alpha_, color='red', linestyle='--',\n",
    "            linewidth=2, label=f'Best α={lasso_cv.alpha_:.4f}')\n",
    "ax2.set_xscale('log')\n",
    "ax2.set_xlabel('Alpha (λ)')\n",
    "ax2.set_ylabel('Cross-validation R² Score')\n",
    "ax2.set_title('Lasso Cross-validation Scores')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# ===== FEATURE IMPORTANCE =====\n",
    "ax3 = axes[2]\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'OLS': np.abs(baseline.coef_),\n",
    "    'Ridge': np.abs(ridge_best.coef_),\n",
    "    'Lasso': np.abs(lasso_best.coef_),\n",
    "    'ElasticNet': np.abs(elastic_best.coef_)\n",
    "})\n",
    "\n",
    "# Berechne durchschnittliche Wichtigkeit\n",
    "feature_importance_mean = feature_importance[['OLS', 'Ridge', 'Lasso', 'ElasticNet']].mean(axis=1)\n",
    "feature_importance['Mean'] = feature_importance_mean\n",
    "feature_importance_sorted = feature_importance.nlargest(10, 'Mean')\n",
    "\n",
    "# Erstelle Barplot\n",
    "feature_importance_sorted.set_index('Feature')[['OLS', 'Ridge', 'Lasso', 'ElasticNet']].plot(\n",
    "    kind='barh', ax=ax3, width=0.8\n",
    ")\n",
    "ax3.set_xlabel('Absolute Coefficient Value')\n",
    "ax3.set_title('Feature Importance Comparison (Top 10)')\n",
    "ax3.legend(title='Model', loc='lower right')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Save\\\\cv_curves_feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Cross-validation analysis completed!\")"
   ],
   "id": "50e1dbb3af4d84e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Statistical Significance Testing",
   "id": "977ab2513f1e353b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STATISTICAL SIGNIFICANCE TESTING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Perform cross-validation for all models to get multiple scores\n",
    "cv_folds = 10\n",
    "print(f\"\\nPerforming {cv_folds}-fold cross-validation for statistical testing...\")\n",
    "\n",
    "cv_scores = {}\n",
    "for model_name, model in models_dict.items():\n",
    "    scores = cross_val_score(model, X_train_scaled, y_train,\n",
    "                            cv=cv_folds, scoring='r2', n_jobs=-1)\n",
    "    cv_scores[model_name] = scores\n",
    "    print(f\"{model_name}: Mean R² = {scores.mean():.4f} ± {scores.std():.4f}\")\n",
    "\n",
    "# Paired t-tests comparing each regularized model to OLS\n",
    "print(\"\\nPaired t-tests (comparing to OLS baseline):\")\n",
    "for model_name in ['Ridge', 'Lasso', 'ElasticNet']:\n",
    "    t_stat, p_value = ttest_rel(cv_scores[model_name], cv_scores['OLS'])\n",
    "    print(f\"\\n{model_name} vs OLS:\")\n",
    "    print(f\"  t-statistic: {t_stat:.4f}\")\n",
    "    print(f\"  p-value: {p_value:.6f}\")\n",
    "    if p_value < 0.05:\n",
    "        if t_stat > 0:\n",
    "            print(f\"  → {model_name} is significantly better than OLS (p < 0.05)\")\n",
    "        else:\n",
    "            print(f\"  → OLS is significantly better than {model_name} (p < 0.05)\")\n",
    "    else:\n",
    "        print(f\"  → No significant difference between {model_name} and OLS (p ≥ 0.05)\")\n",
    "\n",
    "# Bootstrap confidence intervals\n",
    "print(\"\\nBootstrap Confidence Intervals (95% CI for Test R²):\")\n",
    "n_bootstrap = 1000\n",
    "bootstrap_scores = {model: [] for model in models_dict.keys()}\n",
    "\n",
    "for i in range(n_bootstrap):\n",
    "    # Resample test set with replacement\n",
    "    indices = np.random.choice(len(X_test_scaled), len(X_test_scaled), replace=True)\n",
    "    X_boot = X_test_scaled[indices]\n",
    "    y_boot = y_test[indices]\n",
    "\n",
    "    # Calculate R² for each model\n",
    "    bootstrap_scores['OLS'].append(r2_score(y_boot, baseline.predict(X_boot)))\n",
    "    bootstrap_scores['Ridge'].append(r2_score(y_boot, ridge_best.predict(X_boot)))\n",
    "    bootstrap_scores['Lasso'].append(r2_score(y_boot, lasso_best.predict(X_boot)))\n",
    "    bootstrap_scores['ElasticNet'].append(r2_score(y_boot, elastic_best.predict(X_boot)))\n",
    "\n",
    "for model_name in models_dict.keys():\n",
    "    scores = bootstrap_scores[model_name]\n",
    "    ci_lower = np.percentile(scores, 2.5)\n",
    "    ci_upper = np.percentile(scores, 97.5)\n",
    "    print(f\"{model_name}: [{ci_lower:.4f}, {ci_upper:.4f}]\")"
   ],
   "id": "a17f3262dfe4df70",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Result Summary",
   "id": "4bdcffaa5ad664ed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Best model selection\n",
    "test_r2_scores = {\n",
    "    'OLS': comparison_df.loc['OLS', 'Test R²'],\n",
    "    'Ridge': comparison_df.loc['Ridge', 'Test R²'],\n",
    "    'Lasso': comparison_df.loc['Lasso', 'Test R²'],\n",
    "    'ElasticNet': comparison_df.loc['ElasticNet', 'Test R²']\n",
    "}\n",
    "\n",
    "best_model = max(test_r2_scores, key=test_r2_scores.get)\n",
    "best_score = test_r2_scores[best_model]\n",
    "\n",
    "print(f\"\\nBest Model: {best_model}\")\n",
    "print(f\"Test R² Score: {best_score:.4f}\")\n",
    "\n",
    "# Answer to research question\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ANSWER TO RESEARCH QUESTION\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\nDoes regularization materially improve generalization on the diamonds dataset?\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "improvement_ridge = test_r2_scores['Ridge'] - test_r2_scores['OLS']\n",
    "improvement_lasso = test_r2_scores['Lasso'] - test_r2_scores['OLS']\n",
    "improvement_elastic = test_r2_scores['ElasticNet'] - test_r2_scores['OLS']\n",
    "\n",
    "if max(improvement_ridge, improvement_lasso, improvement_elastic) > 0.01:\n",
    "    print(\"YES - Regularization provides meaningful improvement:\")\n",
    "    print(f\"  • Ridge improvement: {improvement_ridge:.4f} ({improvement_ridge/test_r2_scores['OLS']*100:.2f}%)\")\n",
    "    print(f\"  • Lasso improvement: {improvement_lasso:.4f} ({improvement_lasso/test_r2_scores['OLS']*100:.2f}%)\")\n",
    "    print(f\"  • ElasticNet improvement: {improvement_elastic:.4f} ({improvement_elastic/test_r2_scores['OLS']*100:.2f}%)\")\n",
    "else:\n",
    "    print(\"NO - Regularization does not provide substantial improvement\")\n",
    "    print(\"  The improvements are marginal and may not justify the added complexity\")\n",
    "\n",
    "# Why regularization helps (or doesn't help)\n",
    "print(\"\\nWhy regularization helps on this dataset:\")\n",
    "print(\"  1. High multicollinearity between size features (x, y, z, volume, carat)\")\n",
    "print(\"  2. Feature selection by Lasso removes redundant predictors\")\n",
    "print(\"  3. Coefficient shrinkage prevents overfitting to training noise\")\n",
    "print(\"  4. Large dataset size allows regularization to be effective\")\n",
    "\n",
    "# Save results to CSV\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SAVING RESULTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Save model comparison table\n",
    "comparison_df.to_csv('Save\\\\model_comparison_results.csv')\n",
    "print(\"Model comparison saved to 'model_comparison_results.csv'\")\n",
    "\n",
    "# Save feature importance\n",
    "feature_importance.to_csv('Save\\\\feature_importance.csv', index=False)\n",
    "print(\"Feature importance saved to 'feature_importance.csv'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*50)"
   ],
   "id": "89ff0e534dab593f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
