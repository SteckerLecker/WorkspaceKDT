{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Class Imbalance Project - Handle Imbalanced Data Like a Pro!\n",
    "\n",
    "## Welcome to the Real World of Imbalanced Data!\n",
    "\n",
    "Most real-world classification problems have imbalanced classes. Fraud detection, disease diagnosis, spam filtering - they all face this challenge!\n",
    "\n",
    "### What You'll Build:\n",
    "A complete **fraud detection system** using highly imbalanced credit card transaction data, applying multiple techniques to handle the imbalance!\n",
    "\n",
    "### Learning Goals:\n",
    "- Understand why class imbalance is a problem\n",
    "- Learn why accuracy is MISLEADING for imbalanced data\n",
    "- Apply proper evaluation metrics (precision, recall, F1, ROC-AUC, PR-AUC)\n",
    "- Use undersampling techniques\n",
    "- Use oversampling techniques (SMOTE, ADASYN)\n",
    "- Apply class weights\n",
    "- Compare all rebalancing strategies\n",
    "- Calibrate model probabilities\n",
    "- Build production-ready pipeline\n",
    "\n",
    "### Don't Panic!\n",
    "- Read each instruction carefully\n",
    "- Try the TODO exercises yourself first\n",
    "- Hints are provided if you get stuck\n",
    "- Solutions are at the end (but try not to peek!)\n",
    "\n",
    "**Let's tackle class imbalance!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries\n",
    "\n",
    "First, let's import tools including imblearn for resampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report,\n",
    "    roc_auc_score, roc_curve, precision_recall_curve,\n",
    "    average_precision_score\n",
    ")\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "\n",
    "# Imbalanced-learn library for resampling\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN, BorderlineSMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# Make plots look nice\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(\"You're ready to handle imbalanced data!\")\n",
    "print(\"\\nâš ï¸ Make sure you have installed: pip install imbalanced-learn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create Highly Imbalanced Dataset\n",
    "\n",
    "We'll simulate credit card fraud detection data:\n",
    "- **99.7% legitimate** transactions\n",
    "- **0.3% fraudulent** transactions\n",
    "\n",
    "This extreme imbalance is realistic for fraud detection!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create highly imbalanced credit card fraud dataset\n",
    "np.random.seed(42)\n",
    "n_transactions = 10000\n",
    "fraud_ratio = 0.003  # 0.3% fraud rate\n",
    "\n",
    "# Generate features\n",
    "n_features = 28\n",
    "X = np.random.randn(n_transactions, n_features)\n",
    "\n",
    "# Generate imbalanced target\n",
    "n_fraud = int(n_transactions * fraud_ratio)\n",
    "n_legit = n_transactions - n_fraud\n",
    "\n",
    "y = np.array([0] * n_legit + [1] * n_fraud)\n",
    "\n",
    "# Make fraudulent transactions distinguishable (but not perfectly)\n",
    "# Add signal to fraud samples\n",
    "fraud_mask = (y == 1)\n",
    "X[fraud_mask, :5] += np.random.randn(n_fraud, 5) * 3  # Stronger signal in first 5 features\n",
    "X[fraud_mask, 5:10] += np.random.randn(n_fraud, 5) * 2  # Medium signal\n",
    "\n",
    "# Add transaction amounts (feature 0 will be amount)\n",
    "amounts = np.random.exponential(100, n_transactions)\n",
    "amounts[fraud_mask] = np.random.exponential(300, n_fraud)  # Fraud tends to be larger\n",
    "\n",
    "# Create DataFrame\n",
    "feature_names = [f'V{i}' for i in range(1, n_features+1)]\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "df.insert(0, 'Amount', amounts)\n",
    "df['Class'] = y\n",
    "\n",
    "# Shuffle\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Dataset created!\")\n",
    "print(f\"Total transactions: {len(df):,}\")\n",
    "print(f\"Total features: {len(df.columns) - 1}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(df['Class'].value_counts())\n",
    "print(f\"\\nFraud rate: {df['Class'].mean()*100:.3f}%\")\n",
    "print(f\"\\nâš ï¸ This is EXTREMELY imbalanced - typical for fraud detection!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Explore Imbalanced Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 1: Explore and Visualize Class Imbalance\n",
    "\n",
    "1. Display class distribution (counts and percentages)\n",
    "2. Create a bar plot showing the imbalance\n",
    "3. Calculate the imbalance ratio\n",
    "4. Show some statistics for each class\n",
    "\n",
    "ðŸ’¡ **Hint:**\n",
    "```python\n",
    "class_counts = df['Class'].value_counts()\n",
    "class_pct = df['Class'].value_counts(normalize=True) * 100\n",
    "imbalance_ratio = class_counts[0] / class_counts[1]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 1: YOUR CODE HERE\n",
    "# Explore and visualize class imbalance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Split Data with Stratification\n",
    "\n",
    "**CRITICAL:** Always use stratified splitting for imbalanced data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Split Data with Stratification\n",
    "\n",
    "1. Split into train (80%) and test (20%) sets\n",
    "2. Use `stratify=y` to maintain class distribution\n",
    "3. Verify that both sets have similar class distributions\n",
    "\n",
    "ðŸ’¡ **Hint:**\n",
    "```python\n",
    "X = df.drop('Class', axis=1)\n",
    "y = df['Class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "```\n",
    "\n",
    "âš ï¸ **Common Mistake:** Forgetting `stratify` can result in test sets with even fewer fraud cases!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 2: YOUR CODE HERE\n",
    "# Split data with stratification\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Train Baseline Model (No Rebalancing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 3: Train Baseline Model\n",
    "\n",
    "Train a RandomForestClassifier with default parameters:\n",
    "1. No rebalancing yet!\n",
    "2. Fit on training data\n",
    "3. Make predictions on test set\n",
    "\n",
    "ðŸ’¡ **Hint:**\n",
    "```python\n",
    "clf_baseline = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf_baseline.fit(X_train, y_train)\n",
    "y_pred_baseline = clf_baseline.predict(X_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 3: YOUR CODE HERE\n",
    "# Train baseline model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Evaluate with Accuracy (THE WRONG METRIC!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 4: Calculate Accuracy (and understand why it's misleading)\n",
    "\n",
    "1. Calculate accuracy on test set\n",
    "2. Compare with a naive \"always predict majority class\" baseline\n",
    "3. Realize why accuracy is useless here!\n",
    "\n",
    "ðŸ’¡ **Hint:**\n",
    "```python\n",
    "accuracy = accuracy_score(y_test, y_pred_baseline)\n",
    "# Naive baseline: always predict 0 (legitimate)\n",
    "y_pred_naive = np.zeros_like(y_test)\n",
    "accuracy_naive = accuracy_score(y_test, y_pred_naive)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 4: YOUR CODE HERE\n",
    "# Calculate accuracy and understand why it's misleading\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âœ… **Check:** Did you get ~99.7% accuracy? That's TERRIBLE!\n",
    "- A model that predicts \"no fraud\" for everything gets 99.7% accuracy\n",
    "- But it catches ZERO fraud cases!\n",
    "- **Accuracy is useless for imbalanced data!**\n",
    "\n",
    "âš ï¸ **Never use accuracy alone for imbalanced classification!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Evaluate with Proper Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 5: Calculate Proper Metrics for Imbalanced Data\n",
    "\n",
    "Calculate and display:\n",
    "1. **Confusion Matrix**: See actual TP, FP, TN, FN\n",
    "2. **Precision**: Of predicted frauds, how many are correct?\n",
    "3. **Recall**: Of actual frauds, how many did we catch?\n",
    "4. **F1-Score**: Harmonic mean of precision and recall\n",
    "5. **ROC-AUC**: Area under ROC curve\n",
    "6. **Precision-Recall AUC**: Better for imbalanced data!\n",
    "\n",
    "Plot:\n",
    "- Confusion matrix\n",
    "- ROC curve\n",
    "- Precision-Recall curve\n",
    "\n",
    "ðŸ’¡ **Hint:**\n",
    "```python\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(confusion_matrix(y_test, y_pred_baseline))\n",
    "print(classification_report(y_test, y_pred_baseline, target_names=['Legit', 'Fraud']))\n",
    "\n",
    "# For ROC and PR curves, need probability predictions\n",
    "y_pred_proba = clf_baseline.predict_proba(X_test)[:, 1]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 5: YOUR CODE HERE\n",
    "# Calculate and visualize proper metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Apply Random Undersampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 6: Apply Random Undersampling\n",
    "\n",
    "Undersample the majority class to balance the dataset:\n",
    "1. Use RandomUnderSampler from imblearn\n",
    "2. Train model on resampled data\n",
    "3. Evaluate on original test set\n",
    "4. Compare with baseline\n",
    "\n",
    "ðŸ’¡ **Hint:**\n",
    "```python\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_train_rus, y_train_rus = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "clf_rus = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf_rus.fit(X_train_rus, y_train_rus)\n",
    "```\n",
    "\n",
    "âš ï¸ **Important:** Always evaluate on the ORIGINAL test set, not resampled!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 6: YOUR CODE HERE\n",
    "# Apply random undersampling\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Apply Random Oversampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 7: Apply Random Oversampling\n",
    "\n",
    "Oversample the minority class by duplicating samples:\n",
    "1. Use RandomOverSampler\n",
    "2. Train and evaluate\n",
    "3. Compare with undersampling\n",
    "\n",
    "ðŸ’¡ **Hint:**\n",
    "```python\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_train_ros, y_train_ros = ros.fit_resample(X_train, y_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 7: YOUR CODE HERE\n",
    "# Apply random oversampling\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Apply SMOTE (Synthetic Minority Over-sampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 8: Apply SMOTE\n",
    "\n",
    "SMOTE creates synthetic samples instead of duplicating:\n",
    "1. Use SMOTE from imblearn\n",
    "2. Visualize a few synthetic samples (optional)\n",
    "3. Train and evaluate\n",
    "4. Compare with random oversampling\n",
    "\n",
    "ðŸ’¡ **Hint:**\n",
    "```python\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 8: YOUR CODE HERE\n",
    "# Apply SMOTE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Try SMOTE Variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 9: Try SMOTE Variants (ADASYN or BorderlineSMOTE)\n",
    "\n",
    "Try advanced SMOTE variants:\n",
    "1. **ADASYN**: Adaptive Synthetic Sampling\n",
    "2. **BorderlineSMOTE**: Focus on borderline cases\n",
    "\n",
    "Pick one and compare with regular SMOTE!\n",
    "\n",
    "ðŸ’¡ **Hint:**\n",
    "```python\n",
    "from imblearn.over_sampling import ADASYN, BorderlineSMOTE\n",
    "adasyn = ADASYN(random_state=42)\n",
    "X_train_adasyn, y_train_adasyn = adasyn.fit_resample(X_train, y_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 9: YOUR CODE HERE\n",
    "# Try SMOTE variants\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Apply Class Weights (No Resampling!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 10: Use Class Weights\n",
    "\n",
    "Instead of resampling, use class weights to penalize misclassification:\n",
    "1. Train with `class_weight='balanced'`\n",
    "2. No resampling needed!\n",
    "3. Compare with resampling methods\n",
    "\n",
    "ðŸ’¡ **Hint:**\n",
    "```python\n",
    "clf_weighted = RandomForestClassifier(\n",
    "    n_estimators=100, \n",
    "    class_weight='balanced',  # This does the magic!\n",
    "    random_state=42\n",
    ")\n",
    "clf_weighted.fit(X_train, y_train)  # Use original unbalanced data!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 10: YOUR CODE HERE\n",
    "# Apply class weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Comprehensive Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 11: Compare All Methods\n",
    "\n",
    "Create a comprehensive comparison table:\n",
    "1. Baseline (no rebalancing)\n",
    "2. Random Undersampling\n",
    "3. Random Oversampling\n",
    "4. SMOTE\n",
    "5. SMOTE variant (ADASYN/Borderline)\n",
    "6. Class Weights\n",
    "\n",
    "For each, show:\n",
    "- Precision\n",
    "- Recall\n",
    "- F1-Score\n",
    "- ROC-AUC\n",
    "- PR-AUC (Average Precision)\n",
    "\n",
    "Visualize with grouped bar chart!\n",
    "\n",
    "ðŸ’¡ **Hint:**\n",
    "```python\n",
    "results = {\n",
    "    'Method': [],\n",
    "    'Precision': [],\n",
    "    'Recall': [],\n",
    "    'F1': [],\n",
    "    'ROC-AUC': [],\n",
    "    'PR-AUC': []\n",
    "}\n",
    "\n",
    "# For each method:\n",
    "results['Method'].append('Baseline')\n",
    "results['Precision'].append(precision_score(y_test, y_pred_baseline))\n",
    "# ... etc\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 11: YOUR CODE HERE\n",
    "# Create comprehensive comparison\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 14: Calibrate Best Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 12: Calibrate Probability Predictions\n",
    "\n",
    "Choose your best method and calibrate its probabilities:\n",
    "1. Select best model from comparison\n",
    "2. Apply CalibratedClassifierCV\n",
    "3. Create calibration curve\n",
    "4. Compare calibrated vs uncalibrated probabilities\n",
    "\n",
    "ðŸ’¡ **Hint:**\n",
    "```python\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "\n",
    "# Choose best model (e.g., SMOTE)\n",
    "clf_best = clf_smote  # or whichever performed best\n",
    "\n",
    "# Calibrate\n",
    "clf_calibrated = CalibratedClassifierCV(clf_best, method='sigmoid', cv=5)\n",
    "clf_calibrated.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Get probabilities\n",
    "y_proba_uncalib = clf_best.predict_proba(X_test)[:, 1]\n",
    "y_proba_calib = clf_calibrated.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Plot calibration curve\n",
    "fraction_of_positives, mean_predicted_value = calibration_curve(\n",
    "    y_test, y_proba_uncalib, n_bins=10\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 12: YOUR CODE HERE\n",
    "# Calibrate best model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 15: Build Production Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 13: Create Production-Ready Pipeline\n",
    "\n",
    "Build a complete pipeline with:\n",
    "1. Scaling\n",
    "2. Resampling (using imblearn Pipeline)\n",
    "3. Model training\n",
    "\n",
    "ðŸ’¡ **Hint:**\n",
    "```python\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pipeline = ImbPipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('sampler', SMOTE(random_state=42)),  # imblearn Pipeline supports this!\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred_pipeline = pipeline.predict(X_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 13: YOUR CODE HERE\n",
    "# Build production pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 16: Final Evaluation and Business Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 14: Final Evaluation and Business Discussion\n",
    "\n",
    "1. Evaluate final pipeline on test set\n",
    "2. Create comprehensive report with all metrics\n",
    "3. Discuss business implications:\n",
    "   - What's the cost of false positives? (blocking legitimate transaction)\n",
    "   - What's the cost of false negatives? (missing fraud)\n",
    "   - Which metric matters most for this business?\n",
    "4. Recommend threshold adjustment if needed\n",
    "\n",
    "ðŸ’¡ **Hint:**\n",
    "```python\n",
    "# Get probability predictions\n",
    "y_proba_final = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Try different thresholds\n",
    "thresholds = [0.3, 0.5, 0.7]\n",
    "for threshold in thresholds:\n",
    "    y_pred_threshold = (y_proba_final >= threshold).astype(int)\n",
    "    # Calculate metrics\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 14: YOUR CODE HERE\n",
    "# Final evaluation and business interpretation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "### You Did It!\n",
    "\n",
    "You just:\n",
    "- âœ… Understood why class imbalance is challenging\n",
    "- âœ… Learned why accuracy is misleading\n",
    "- âœ… Applied proper evaluation metrics\n",
    "- âœ… Used undersampling (RandomUnderSampler)\n",
    "- âœ… Used oversampling (RandomOverSampler, SMOTE, ADASYN)\n",
    "- âœ… Applied class weights\n",
    "- âœ… Compared all rebalancing strategies\n",
    "- âœ… Calibrated model probabilities\n",
    "- âœ… Built production-ready pipeline\n",
    "- âœ… Discussed business implications\n",
    "\n",
    "### What You Learned:\n",
    "\n",
    "**1. The Problem:**\n",
    "- Imbalanced data is common in real world (fraud, disease, spam)\n",
    "- Accuracy is USELESS - can get 99% by predicting majority class!\n",
    "- Need proper metrics: precision, recall, F1, ROC-AUC, PR-AUC\n",
    "\n",
    "**2. Evaluation Metrics:**\n",
    "- **Precision**: Of predicted positives, how many are correct?\n",
    "- **Recall**: Of actual positives, how many did we find?\n",
    "- **F1**: Harmonic mean of precision and recall\n",
    "- **ROC-AUC**: Good general metric\n",
    "- **PR-AUC**: Better for very imbalanced data!\n",
    "\n",
    "**3. Rebalancing Techniques:**\n",
    "- **Undersampling**: Fast but loses data\n",
    "- **Random Oversampling**: Simple but risk of overfitting\n",
    "- **SMOTE**: Creates synthetic samples, usually best\n",
    "- **ADASYN**: Adaptive, focuses on hard cases\n",
    "- **Class Weights**: No resampling needed, often effective\n",
    "\n",
    "**4. Which Method to Use?**\n",
    "- **Large dataset**: Try undersampling first (fast)\n",
    "- **Small dataset**: Use SMOTE or class weights\n",
    "- **Very imbalanced**: SMOTE + class weights combined\n",
    "- **Production**: Use pipelines for reproducibility\n",
    "\n",
    "**5. Business Considerations:**\n",
    "- Cost of false positive vs false negative\n",
    "- Adjust threshold based on business needs\n",
    "- Monitor model performance over time\n",
    "- Consider ensemble of different rebalancing methods\n",
    "\n",
    "### Key Insights:\n",
    "- NEVER use accuracy alone for imbalanced data\n",
    "- Stratified splitting is CRITICAL\n",
    "- Always evaluate on original (imbalanced) test set\n",
    "- PR-AUC often better than ROC-AUC for extreme imbalance\n",
    "- Class weights are underrated - try them first!\n",
    "- Business context determines which metric matters most\n",
    "\n",
    "### Best Practices:\n",
    "1. Always check class distribution first\n",
    "2. Use stratified train/test split\n",
    "3. Try multiple rebalancing techniques\n",
    "4. Use proper metrics (NOT accuracy!)\n",
    "5. Plot ROC and PR curves\n",
    "6. Calibrate probabilities for deployment\n",
    "7. Consider business costs\n",
    "8. Use pipelines for production\n",
    "\n",
    "### Next Steps:\n",
    "- Apply to real imbalanced datasets (Kaggle has many!)\n",
    "- Try ensemble methods (EasyEnsemble, BalancedBaggingClassifier)\n",
    "- Experiment with cost-sensitive learning\n",
    "- Learn about anomaly detection approaches\n",
    "\n",
    "---\n",
    "\n",
    "## Extension Exercises (Optional, Harder!)\n",
    "\n",
    "1. **Cost-Sensitive Learning**: Implement custom loss function with different costs for FP and FN\n",
    "2. **Ensemble of Samplers**: Combine multiple resampling strategies\n",
    "3. **Threshold Optimization**: Find optimal threshold using business costs\n",
    "4. **One-Class Classification**: Try Isolation Forest or One-Class SVM\n",
    "5. **Advanced Techniques**: Try SMOTE-ENN, SMOTE-Tomek\n",
    "6. **Real Dataset**: Apply to Kaggle Credit Card Fraud dataset\n",
    "\n",
    "---\n",
    "\n",
    "## You're an Imbalanced Data Expert Now!\n",
    "\n",
    "**You just mastered handling imbalanced classification from scratch!**\n",
    "\n",
    "**That's AMAZING! Keep balancing those datasets!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Solutions (Only Look After Trying!)\n",
    "\n",
    "Here are the solutions to all TODOs. But remember: **you learn by doing, not by copying!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION TO TODO 1\n",
    "print(\"Class Distribution Analysis\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "class_counts = df['Class'].value_counts()\n",
    "class_pct = df['Class'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"\\nCounts:\")\n",
    "print(class_counts)\n",
    "print(\"\\nPercentages:\")\n",
    "print(class_pct)\n",
    "\n",
    "imbalance_ratio = class_counts[0] / class_counts[1]\n",
    "print(f\"\\nImbalance Ratio: {imbalance_ratio:.1f}:1\")\n",
    "print(f\"(Majority class is {imbalance_ratio:.0f}x larger than minority class)\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar plot\n",
    "class_counts.plot(kind='bar', ax=axes[0], color=['steelblue', 'coral'])\n",
    "axes[0].set_title('Class Distribution (Count)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Class (0=Legit, 1=Fraud)')\n",
    "axes[0].set_ylabel('Number of Transactions')\n",
    "axes[0].set_xticklabels(['Legitimate', 'Fraud'], rotation=0)\n",
    "for i, v in enumerate(class_counts):\n",
    "    axes[0].text(i, v + 100, f'{v:,}', ha='center', fontweight='bold')\n",
    "\n",
    "# Pie chart\n",
    "axes[1].pie(class_counts, labels=['Legitimate', 'Fraud'], autopct='%1.3f%%',\n",
    "           colors=['steelblue', 'coral'], startangle=90)\n",
    "axes[1].set_title('Class Distribution (Percentage)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistics by class\n",
    "print(\"\\nStatistics by Class:\")\n",
    "print(df.groupby('Class')['Amount'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION TO TODO 2\n",
    "print(\"Splitting data with stratification...\\n\")\n",
    "\n",
    "X = df.drop('Class', axis=1)\n",
    "y = df['Class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Split completed!\")\n",
    "print(f\"Training set: {len(X_train)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")\n",
    "\n",
    "print(\"\\nClass distribution in training set:\")\n",
    "print(y_train.value_counts())\n",
    "print(f\"Fraud rate: {y_train.mean()*100:.3f}%\")\n",
    "\n",
    "print(\"\\nClass distribution in test set:\")\n",
    "print(y_test.value_counts())\n",
    "print(f\"Fraud rate: {y_test.mean()*100:.3f}%\")\n",
    "\n",
    "print(\"\\nâœ… Both sets have similar fraud rates - stratification worked!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION TO TODO 3\n",
    "print(\"Training baseline model (no rebalancing)...\\n\")\n",
    "\n",
    "clf_baseline = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "clf_baseline.fit(X_train, y_train)\n",
    "y_pred_baseline = clf_baseline.predict(X_test)\n",
    "\n",
    "print(\"âœ… Baseline model trained!\")\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION TO TODO 4\n",
    "print(\"Evaluating with ACCURACY (THE WRONG METRIC!)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_baseline)\n",
    "print(f\"\\nModel Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "\n",
    "# Naive baseline\n",
    "y_pred_naive = np.zeros_like(y_test)\n",
    "accuracy_naive = accuracy_score(y_test, y_pred_naive)\n",
    "print(f\"Naive 'Always Predict 0' Accuracy: {accuracy_naive:.4f} ({accuracy_naive*100:.2f}%)\")\n",
    "\n",
    "print(\"\\nâš ï¸ PROBLEM IDENTIFIED:\")\n",
    "print(\"   Both models have ~99.7% accuracy!\")\n",
    "print(\"   But the naive model catches ZERO fraud cases!\")\n",
    "print(\"   This proves accuracy is USELESS for imbalanced data!\")\n",
    "\n",
    "# Show predictions\n",
    "print(\"\\nBaseline model predictions:\")\n",
    "print(pd.Series(y_pred_baseline).value_counts())\n",
    "print(f\"\\nFraud cases predicted: {y_pred_baseline.sum()}\")\n",
    "print(f\"Actual fraud cases: {y_test.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION TO TODO 5\n",
    "print(\"Evaluating with PROPER METRICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_baseline)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "print(\"\\n[TN  FP]\")\n",
    "print(\"[FN  TP]\")\n",
    "\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(f\"\\nTrue Negatives (correct legit): {tn}\")\n",
    "print(f\"False Positives (legit flagged as fraud): {fp}\")\n",
    "print(f\"False Negatives (fraud missed): {fn}\")\n",
    "print(f\"True Positives (fraud caught): {tp}\")\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_baseline, target_names=['Legitimate', 'Fraud']))\n",
    "\n",
    "# Calculate metrics\n",
    "precision = precision_score(y_test, y_pred_baseline)\n",
    "recall = recall_score(y_test, y_pred_baseline)\n",
    "f1 = f1_score(y_test, y_pred_baseline)\n",
    "\n",
    "print(f\"\\nKey Metrics:\")\n",
    "print(f\"  Precision: {precision:.4f} (of predicted frauds, {precision*100:.1f}% are correct)\")\n",
    "print(f\"  Recall: {recall:.4f} (we catch {recall*100:.1f}% of actual frauds)\")\n",
    "print(f\"  F1-Score: {f1:.4f} (harmonic mean)\")\n",
    "\n",
    "# ROC and PR curves\n",
    "y_pred_proba_baseline = clf_baseline.predict_proba(X_test)[:, 1]\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba_baseline)\n",
    "pr_auc = average_precision_score(y_test, y_pred_proba_baseline)\n",
    "\n",
    "print(f\"  ROC-AUC: {roc_auc:.4f}\")\n",
    "print(f\"  PR-AUC: {pr_auc:.4f}\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# 1. Confusion Matrix\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "           xticklabels=['Legit', 'Fraud'], yticklabels=['Legit', 'Fraud'])\n",
    "axes[0].set_title('Confusion Matrix')\n",
    "axes[0].set_ylabel('Actual')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "\n",
    "# 2. ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba_baseline)\n",
    "axes[1].plot(fpr, tpr, linewidth=2, label=f'ROC (AUC={roc_auc:.3f})')\n",
    "axes[1].plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "axes[1].set_xlabel('False Positive Rate')\n",
    "axes[1].set_ylabel('True Positive Rate')\n",
    "axes[1].set_title('ROC Curve')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Precision-Recall Curve\n",
    "precision_curve, recall_curve, _ = precision_recall_curve(y_test, y_pred_proba_baseline)\n",
    "axes[2].plot(recall_curve, precision_curve, linewidth=2, label=f'PR (AUC={pr_auc:.3f})')\n",
    "axes[2].axhline(y=y_test.mean(), color='k', linestyle='--', label='Baseline')\n",
    "axes[2].set_xlabel('Recall')\n",
    "axes[2].set_ylabel('Precision')\n",
    "axes[2].set_title('Precision-Recall Curve')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… These metrics actually tell us how well we're catching fraud!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION TO TODO 6\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "print(\"Applying Random Undersampling...\\n\")\n",
    "\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_train_rus, y_train_rus = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f\"Original training set: {len(X_train)} samples\")\n",
    "print(f\"After undersampling: {len(X_train_rus)} samples\")\n",
    "print(f\"Data loss: {len(X_train) - len(X_train_rus)} samples ({(1-len(X_train_rus)/len(X_train))*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nClass distribution after undersampling:\")\n",
    "print(pd.Series(y_train_rus).value_counts())\n",
    "\n",
    "# Train model\n",
    "clf_rus = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "clf_rus.fit(X_train_rus, y_train_rus)\n",
    "y_pred_rus = clf_rus.predict(X_test)\n",
    "y_proba_rus = clf_rus.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\nPerformance on test set:\")\n",
    "print(f\"  Precision: {precision_score(y_test, y_pred_rus):.4f}\")\n",
    "print(f\"  Recall: {recall_score(y_test, y_pred_rus):.4f}\")\n",
    "print(f\"  F1-Score: {f1_score(y_test, y_pred_rus):.4f}\")\n",
    "print(f\"  ROC-AUC: {roc_auc_score(y_test, y_proba_rus):.4f}\")\n",
    "\n",
    "print(\"\\nâœ… Undersampling balances classes but loses data!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION TO TODO 7\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "print(\"Applying Random Oversampling...\\n\")\n",
    "\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_train_ros, y_train_ros = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f\"Original training set: {len(X_train)} samples\")\n",
    "print(f\"After oversampling: {len(X_train_ros)} samples\")\n",
    "print(f\"Samples added: {len(X_train_ros) - len(X_train)} ({(len(X_train_ros)/len(X_train)-1)*100:.1f}% increase)\")\n",
    "\n",
    "print(\"\\nClass distribution after oversampling:\")\n",
    "print(pd.Series(y_train_ros).value_counts())\n",
    "\n",
    "# Train model\n",
    "clf_ros = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "clf_ros.fit(X_train_ros, y_train_ros)\n",
    "y_pred_ros = clf_ros.predict(X_test)\n",
    "y_proba_ros = clf_ros.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\nPerformance on test set:\")\n",
    "print(f\"  Precision: {precision_score(y_test, y_pred_ros):.4f}\")\n",
    "print(f\"  Recall: {recall_score(y_test, y_pred_ros):.4f}\")\n",
    "print(f\"  F1-Score: {f1_score(y_test, y_pred_ros):.4f}\")\n",
    "print(f\"  ROC-AUC: {roc_auc_score(y_test, y_proba_ros):.4f}\")\n",
    "\n",
    "print(\"\\nâœ… Oversampling duplicates minority class samples!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION TO TODO 8\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "print(\"Applying SMOTE (Synthetic Minority Over-sampling)...\\n\")\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f\"Original training set: {len(X_train)} samples\")\n",
    "print(f\"After SMOTE: {len(X_train_smote)} samples\")\n",
    "print(f\"Synthetic samples created: {len(X_train_smote) - len(X_train)}\")\n",
    "\n",
    "print(\"\\nClass distribution after SMOTE:\")\n",
    "print(pd.Series(y_train_smote).value_counts())\n",
    "\n",
    "# Train model\n",
    "clf_smote = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "clf_smote.fit(X_train_smote, y_train_smote)\n",
    "y_pred_smote = clf_smote.predict(X_test)\n",
    "y_proba_smote = clf_smote.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\nPerformance on test set:\")\n",
    "print(f\"  Precision: {precision_score(y_test, y_pred_smote):.4f}\")\n",
    "print(f\"  Recall: {recall_score(y_test, y_pred_smote):.4f}\")\n",
    "print(f\"  F1-Score: {f1_score(y_test, y_pred_smote):.4f}\")\n",
    "print(f\"  ROC-AUC: {roc_auc_score(y_test, y_proba_smote):.4f}\")\n",
    "\n",
    "print(\"\\nâœ… SMOTE creates synthetic samples instead of duplicating!\")\n",
    "print(\"   Often better than random oversampling.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION TO TODO 9\n",
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "print(\"Applying ADASYN (Adaptive Synthetic Sampling)...\\n\")\n",
    "\n",
    "adasyn = ADASYN(random_state=42)\n",
    "X_train_adasyn, y_train_adasyn = adasyn.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f\"Original training set: {len(X_train)} samples\")\n",
    "print(f\"After ADASYN: {len(X_train_adasyn)} samples\")\n",
    "\n",
    "print(\"\\nClass distribution after ADASYN:\")\n",
    "print(pd.Series(y_train_adasyn).value_counts())\n",
    "\n",
    "# Train model\n",
    "clf_adasyn = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "clf_adasyn.fit(X_train_adasyn, y_train_adasyn)\n",
    "y_pred_adasyn = clf_adasyn.predict(X_test)\n",
    "y_proba_adasyn = clf_adasyn.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\nPerformance on test set:\")\n",
    "print(f\"  Precision: {precision_score(y_test, y_pred_adasyn):.4f}\")\n",
    "print(f\"  Recall: {recall_score(y_test, y_pred_adasyn):.4f}\")\n",
    "print(f\"  F1-Score: {f1_score(y_test, y_pred_adasyn):.4f}\")\n",
    "print(f\"  ROC-AUC: {roc_auc_score(y_test, y_proba_adasyn):.4f}\")\n",
    "\n",
    "print(\"\\nâœ… ADASYN focuses on hard-to-learn examples!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION TO TODO 10\n",
    "print(\"Applying Class Weights (No Resampling!)...\\n\")\n",
    "\n",
    "clf_weighted = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    class_weight='balanced',  # This handles imbalance!\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train on ORIGINAL unbalanced data\n",
    "clf_weighted.fit(X_train, y_train)\n",
    "y_pred_weighted = clf_weighted.predict(X_test)\n",
    "y_proba_weighted = clf_weighted.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"Trained on original training set (no resampling!)\")\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\nPerformance on test set:\")\n",
    "print(f\"  Precision: {precision_score(y_test, y_pred_weighted):.4f}\")\n",
    "print(f\"  Recall: {recall_score(y_test, y_pred_weighted):.4f}\")\n",
    "print(f\"  F1-Score: {f1_score(y_test, y_pred_weighted):.4f}\")\n",
    "print(f\"  ROC-AUC: {roc_auc_score(y_test, y_proba_weighted):.4f}\")\n",
    "\n",
    "print(\"\\nâœ… Class weights handle imbalance internally - no resampling needed!\")\n",
    "print(\"   Often competitive with resampling methods.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION TO TODO 11\n",
    "print(\"COMPREHENSIVE COMPARISON OF ALL METHODS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Collect all results\n",
    "methods_data = {\n",
    "    'Baseline': (y_pred_baseline, y_proba_baseline),\n",
    "    'Undersampling': (y_pred_rus, y_proba_rus),\n",
    "    'Oversampling': (y_pred_ros, y_proba_ros),\n",
    "    'SMOTE': (y_pred_smote, y_proba_smote),\n",
    "    'ADASYN': (y_pred_adasyn, y_proba_adasyn),\n",
    "    'Class Weights': (y_pred_weighted, y_proba_weighted)\n",
    "}\n",
    "\n",
    "results = {\n",
    "    'Method': [],\n",
    "    'Precision': [],\n",
    "    'Recall': [],\n",
    "    'F1': [],\n",
    "    'ROC-AUC': [],\n",
    "    'PR-AUC': []\n",
    "}\n",
    "\n",
    "for method, (y_pred, y_proba) in methods_data.items():\n",
    "    results['Method'].append(method)\n",
    "    results['Precision'].append(precision_score(y_test, y_pred))\n",
    "    results['Recall'].append(recall_score(y_test, y_pred))\n",
    "    results['F1'].append(f1_score(y_test, y_pred))\n",
    "    results['ROC-AUC'].append(roc_auc_score(y_test, y_proba))\n",
    "    results['PR-AUC'].append(average_precision_score(y_test, y_proba))\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n\", results_df.to_string(index=False))\n",
    "\n",
    "# Find best method\n",
    "best_f1_idx = results_df['F1'].idxmax()\n",
    "best_method = results_df.loc[best_f1_idx, 'Method']\n",
    "print(f\"\\nðŸ† Best F1-Score: {best_method} ({results_df.loc[best_f1_idx, 'F1']:.4f})\")\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "metrics = ['Precision', 'Recall', 'F1', 'ROC-AUC', 'PR-AUC']\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, len(results_df)))\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    axes[idx].bar(results_df['Method'], results_df[metric], color=colors)\n",
    "    axes[idx].set_title(metric, fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Score')\n",
    "    axes[idx].set_xticklabels(results_df['Method'], rotation=45, ha='right')\n",
    "    axes[idx].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Highlight best\n",
    "    best_idx = results_df[metric].idxmax()\n",
    "    axes[idx].bar(results_df.loc[best_idx, 'Method'], results_df.loc[best_idx, metric],\n",
    "                 color='gold', edgecolor='black', linewidth=2)\n",
    "\n",
    "# Overall comparison\n",
    "axes[5].axis('off')\n",
    "summary_text = f\"\"\"Summary:\n",
    "\n",
    "Best Overall: {best_method}\n",
    "Best F1: {results_df['F1'].max():.4f}\n",
    "Best Recall: {results_df['Recall'].max():.4f}\n",
    "Best Precision: {results_df['Precision'].max():.4f}\n",
    "\n",
    "Key Insights:\n",
    "â€¢ Baseline has high precision, low recall\n",
    "â€¢ Resampling improves recall significantly\n",
    "â€¢ SMOTE usually balances precision/recall well\n",
    "â€¢ Class weights are competitive\n",
    "\"\"\"\n",
    "axes[5].text(0.1, 0.5, summary_text, fontsize=11, verticalalignment='center',\n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.suptitle('Comparison of Rebalancing Methods', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Comprehensive comparison complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION TO TODO 12\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "\n",
    "print(\"Calibrating best model...\\n\")\n",
    "\n",
    "# Choose best model (e.g., SMOTE based on comparison)\n",
    "best_clf = clf_smote\n",
    "X_train_best = X_train_smote\n",
    "y_train_best = y_train_smote\n",
    "\n",
    "print(f\"Using SMOTE model for calibration...\")\n",
    "\n",
    "# Calibrate\n",
    "clf_calibrated = CalibratedClassifierCV(best_clf, method='sigmoid', cv=3)\n",
    "clf_calibrated.fit(X_train_best, y_train_best)\n",
    "\n",
    "# Get probabilities\n",
    "y_proba_uncalib = best_clf.predict_proba(X_test)[:, 1]\n",
    "y_proba_calib = clf_calibrated.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"âœ… Calibration complete!\\n\")\n",
    "\n",
    "# Evaluate calibration\n",
    "print(\"Comparing uncalibrated vs calibrated:\")\n",
    "print(f\"  Uncalibrated ROC-AUC: {roc_auc_score(y_test, y_proba_uncalib):.4f}\")\n",
    "print(f\"  Calibrated ROC-AUC: {roc_auc_score(y_test, y_proba_calib):.4f}\")\n",
    "\n",
    "# Plot calibration curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 1. Calibration curve\n",
    "fraction_pos_uncalib, mean_pred_uncalib = calibration_curve(\n",
    "    y_test, y_proba_uncalib, n_bins=10, strategy='uniform'\n",
    ")\n",
    "fraction_pos_calib, mean_pred_calib = calibration_curve(\n",
    "    y_test, y_proba_calib, n_bins=10, strategy='uniform'\n",
    ")\n",
    "\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', label='Perfect Calibration')\n",
    "axes[0].plot(mean_pred_uncalib, fraction_pos_uncalib, 's-', label='Uncalibrated', linewidth=2)\n",
    "axes[0].plot(mean_pred_calib, fraction_pos_calib, 'o-', label='Calibrated', linewidth=2)\n",
    "axes[0].set_xlabel('Mean Predicted Probability')\n",
    "axes[0].set_ylabel('Fraction of Positives')\n",
    "axes[0].set_title('Calibration Curve')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Probability histograms\n",
    "axes[1].hist(y_proba_uncalib[y_test==0], bins=50, alpha=0.5, label='Uncalib Legit', color='blue')\n",
    "axes[1].hist(y_proba_uncalib[y_test==1], bins=50, alpha=0.5, label='Uncalib Fraud', color='red')\n",
    "axes[1].hist(y_proba_calib[y_test==0], bins=50, alpha=0.3, label='Calib Legit', color='cyan')\n",
    "axes[1].hist(y_proba_calib[y_test==1], bins=50, alpha=0.3, label='Calib Fraud', color='orange')\n",
    "axes[1].set_xlabel('Predicted Probability')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('Probability Distributions')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Calibration ensures predicted probabilities are reliable!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION TO TODO 13\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"Building production-ready pipeline...\\n\")\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = ImbPipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('sampler', SMOTE(random_state=42)),\n",
    "    ('classifier', RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "print(\"Pipeline structure:\")\n",
    "print(pipeline)\n",
    "\n",
    "# Train pipeline\n",
    "print(\"\\nTraining pipeline...\")\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_pipeline = pipeline.predict(X_test)\n",
    "y_proba_pipeline = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\nâœ… Pipeline trained successfully!\")\n",
    "print(\"\\nPerformance:\")\n",
    "print(f\"  Precision: {precision_score(y_test, y_pred_pipeline):.4f}\")\n",
    "print(f\"  Recall: {recall_score(y_test, y_pred_pipeline):.4f}\")\n",
    "print(f\"  F1-Score: {f1_score(y_test, y_pred_pipeline):.4f}\")\n",
    "print(f\"  ROC-AUC: {roc_auc_score(y_test, y_proba_pipeline):.4f}\")\n",
    "\n",
    "print(\"\\nâœ… Pipeline is ready for production!\")\n",
    "print(\"   Benefits:\")\n",
    "print(\"   â€¢ Consistent preprocessing\")\n",
    "print(\"   â€¢ Prevents data leakage\")\n",
    "print(\"   â€¢ Easy to deploy\")\n",
    "print(\"   â€¢ Can be saved with joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION TO TODO 14\n",
    "print(\"FINAL EVALUATION AND BUSINESS INTERPRETATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get probability predictions\n",
    "y_proba_final = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Try different thresholds\n",
    "print(\"\\nThreshold Analysis:\")\n",
    "print(\"-\" * 80)\n",
    "thresholds = [0.3, 0.5, 0.7, 0.9]\n",
    "\n",
    "threshold_results = []\n",
    "for threshold in thresholds:\n",
    "    y_pred_threshold = (y_proba_final >= threshold).astype(int)\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred_threshold)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    prec = precision_score(y_test, y_pred_threshold, zero_division=0)\n",
    "    rec = recall_score(y_test, y_pred_threshold)\n",
    "    f1 = f1_score(y_test, y_pred_threshold)\n",
    "    \n",
    "    threshold_results.append({\n",
    "        'Threshold': threshold,\n",
    "        'TP': tp,\n",
    "        'FP': fp,\n",
    "        'FN': fn,\n",
    "        'TN': tn,\n",
    "        'Precision': prec,\n",
    "        'Recall': rec,\n",
    "        'F1': f1\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nThreshold = {threshold}\")\n",
    "    print(f\"  True Positives (fraud caught): {tp}\")\n",
    "    print(f\"  False Positives (legit flagged): {fp}\")\n",
    "    print(f\"  False Negatives (fraud missed): {fn}\")\n",
    "    print(f\"  Precision: {prec:.4f}, Recall: {rec:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "threshold_df = pd.DataFrame(threshold_results)\n",
    "\n",
    "# Visualize threshold impact\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(threshold_df['Threshold'], threshold_df['Precision'], 'o-', label='Precision', linewidth=2)\n",
    "axes[0].plot(threshold_df['Threshold'], threshold_df['Recall'], 's-', label='Recall', linewidth=2)\n",
    "axes[0].plot(threshold_df['Threshold'], threshold_df['F1'], '^-', label='F1', linewidth=2)\n",
    "axes[0].set_xlabel('Threshold')\n",
    "axes[0].set_ylabel('Score')\n",
    "axes[0].set_title('Metrics vs Threshold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "x = np.arange(len(thresholds))\n",
    "width = 0.2\n",
    "axes[1].bar(x - width, threshold_df['TP'], width, label='True Pos', color='green')\n",
    "axes[1].bar(x, threshold_df['FP'], width, label='False Pos', color='orange')\n",
    "axes[1].bar(x + width, threshold_df['FN'], width, label='False Neg', color='red')\n",
    "axes[1].set_xlabel('Threshold')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('Confusion Matrix Components')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(thresholds)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Business interpretation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BUSINESS INTERPRETATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "ðŸ’° Cost Analysis:\n",
    "\n",
    "False Positive (FP): Legitimate transaction flagged as fraud\n",
    "  â€¢ Customer inconvenience\n",
    "  â€¢ Manual review cost (~$10)\n",
    "  â€¢ Potential customer churn\n",
    "  Cost: ~$10 - $50 per FP\n",
    "\n",
    "False Negative (FN): Fraud transaction missed\n",
    "  â€¢ Direct financial loss (avg transaction ~$300)\n",
    "  â€¢ Investigation costs\n",
    "  â€¢ Reputation damage\n",
    "  Cost: ~$300 - $1000 per FN\n",
    "\n",
    "ðŸ“Š Recommendations:\n",
    "\n",
    "1. For Conservative Approach (minimize fraud loss):\n",
    "   â€¢ Use LOWER threshold (0.3)\n",
    "   â€¢ Maximizes recall (catch more fraud)\n",
    "   â€¢ More false positives (more reviews)\n",
    "   â€¢ Best when fraud cost >> review cost\n",
    "\n",
    "2. For Balanced Approach:\n",
    "   â€¢ Use threshold ~0.5\n",
    "   â€¢ Balance precision and recall\n",
    "   â€¢ F1-score optimization\n",
    "   â€¢ Good starting point\n",
    "\n",
    "3. For Customer Experience Focus:\n",
    "   â€¢ Use HIGHER threshold (0.7-0.9)\n",
    "   â€¢ Minimize false positives\n",
    "   â€¢ Accept some fraud losses\n",
    "   â€¢ Best when customer satisfaction critical\n",
    "\n",
    "ðŸŽ¯ Suggested Strategy:\n",
    "   Start with threshold = 0.5\n",
    "   Monitor false positive rate\n",
    "   Adjust based on business metrics\n",
    "   A/B test different thresholds\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nâœ… Final model is ready for deployment!\")\n",
    "print(f\"\\nFinal Performance (threshold=0.5):\")\n",
    "final_pred = (y_proba_final >= 0.5).astype(int)\n",
    "print(classification_report(y_test, final_pred, target_names=['Legitimate', 'Fraud']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
