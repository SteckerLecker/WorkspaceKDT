{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Feature Engineering Workshop - Create Better Features!\n",
    "\n",
    "## Welcome to Feature Engineering!\n",
    "\n",
    "This is where you'll learn the art and science of creating new features from existing data to boost model performance!\n",
    "\n",
    "### What You'll Build:\n",
    "A complete **feature engineering pipeline** for a customer churn dataset, transforming raw data into powerful predictive features!\n",
    "\n",
    "### Learning Goals:\n",
    "- Create domain knowledge features\n",
    "- Apply mathematical transformations\n",
    "- Extract datetime features\n",
    "- Normalize and scale features\n",
    "- Encode categorical variables (one-hot, label, target, frequency)\n",
    "- Create interaction features\n",
    "- Build automated feature engineering pipeline\n",
    "- Evaluate feature importance\n",
    "\n",
    "### Don't Panic!\n",
    "- Read each instruction carefully\n",
    "- Try the TODO exercises yourself first\n",
    "- Hints are provided if you get stuck\n",
    "- Solutions are at the end (but try not to peek!)\n",
    "\n",
    "**Let's engineer some amazing features!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries\n",
    "\n",
    "First, let's import the tools we need for feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from scipy import stats\n",
    "from scipy.special import boxcox\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Make plots look nice\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(\"You're ready to engineer some features!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create the Dataset\n",
    "\n",
    "We'll create a **realistic telecom customer churn dataset** with rich features for engineering!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create telecom customer churn dataset\n",
    "np.random.seed(42)\n",
    "n_customers = 1000\n",
    "\n",
    "# Generate base date range\n",
    "base_date = pd.Timestamp('2024-01-01')\n",
    "signup_dates = [base_date - timedelta(days=int(x)) for x in np.random.uniform(30, 730, n_customers)]\n",
    "\n",
    "data = {\n",
    "    'customer_id': range(1, n_customers + 1),\n",
    "    'signup_date': signup_dates,\n",
    "    'age': np.random.normal(45, 15, n_customers).clip(18, 85).astype(int),\n",
    "    'tenure_months': np.random.normal(24, 15, n_customers).clip(1, 72).astype(int),\n",
    "    'monthly_charges': np.random.gamma(5, 15, n_customers).clip(20, 150),\n",
    "    'total_charges': None,  # Will calculate from monthly * tenure\n",
    "    'contract_type': np.random.choice(['Month-to-month', 'One year', 'Two year'], n_customers, p=[0.55, 0.25, 0.20]),\n",
    "    'payment_method': np.random.choice(['Credit card', 'Bank transfer', 'Electronic check', 'Mailed check'], n_customers, p=[0.3, 0.25, 0.3, 0.15]),\n",
    "    'internet_service': np.random.choice(['DSL', 'Fiber optic', 'No'], n_customers, p=[0.35, 0.45, 0.20]),\n",
    "    'online_security': np.random.choice(['Yes', 'No'], n_customers),\n",
    "    'tech_support': np.random.choice(['Yes', 'No'], n_customers),\n",
    "    'streaming_tv': np.random.choice(['Yes', 'No'], n_customers),\n",
    "    'streaming_movies': np.random.choice(['Yes', 'No'], n_customers),\n",
    "    'paperless_billing': np.random.choice(['Yes', 'No'], n_customers, p=[0.6, 0.4]),\n",
    "    'num_support_calls': np.random.poisson(2.5, n_customers),\n",
    "    'num_tech_tickets': np.random.poisson(1.5, n_customers),\n",
    "    'num_admin_tickets': np.random.poisson(1.0, n_customers),\n",
    "    'avg_session_duration_min': np.random.gamma(3, 20, n_customers).clip(5, 200),\n",
    "    'data_usage_gb': np.random.gamma(2, 15, n_customers).clip(1, 150),\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate total charges\n",
    "df['total_charges'] = df['monthly_charges'] * df['tenure_months'] + np.random.normal(0, 100, n_customers)\n",
    "df['total_charges'] = df['total_charges'].clip(20, 10000)\n",
    "\n",
    "# Add last payment date\n",
    "df['last_payment_date'] = pd.to_datetime('2024-01-01') - pd.to_timedelta(np.random.randint(0, 60, n_customers), unit='D')\n",
    "\n",
    "# Generate churn based on features (realistic patterns)\n",
    "churn_prob = (\n",
    "    0.05 +  # base probability\n",
    "    0.35 * (df['contract_type'] == 'Month-to-month') +\n",
    "    0.25 * (df['tenure_months'] < 12) +\n",
    "    0.15 * (df['num_support_calls'] > 4) +\n",
    "    0.10 * (df['payment_method'] == 'Electronic check') +\n",
    "    0.10 * (df['paperless_billing'] == 'No') +\n",
    "    0.05 * (df['tech_support'] == 'No')\n",
    ")\n",
    "df['churn'] = (np.random.random(n_customers) < churn_prob.clip(0, 0.95)).astype(int)\n",
    "\n",
    "print(f\"Dataset created!\")\n",
    "print(f\"Total customers: {len(df)}\")\n",
    "print(f\"Total features: {len(df.columns) - 1}  (excluding target)\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(f\"  Churned: {df['churn'].sum()} ({df['churn'].mean()*100:.1f}%)\")\n",
    "print(f\"  Stayed:  {len(df) - df['churn'].sum()} ({(1-df['churn'].mean())*100:.1f}%)\")\n",
    "print(f\"\\nThis dataset is perfect for feature engineering!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Explore the Dataset (ALWAYS DO THIS FIRST!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 1: Explore the Dataset\n",
    "\n",
    "Display:\n",
    "1. First 10 rows\n",
    "2. Data types and info\n",
    "3. Identify numerical and categorical features\n",
    "4. Basic statistics\n",
    "\n",
    "💡 **Hint:** \n",
    "```python\n",
    "df.head(10)\n",
    "df.info()\n",
    "numerical_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = df.select_dtypes(include=['object']).columns.tolist()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 1: YOUR CODE HERE\n",
    "# Explore the dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create Domain Knowledge Features\n",
    "\n",
    "Use your understanding of the business to create meaningful features!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Create Domain-Specific Features\n",
    "\n",
    "Create these business-meaningful features:\n",
    "1. `tenure_years`: Convert tenure_months to years\n",
    "2. `avg_monthly_charges`: total_charges / tenure_months\n",
    "3. `charges_per_call`: monthly_charges / (num_support_calls + 1) to avoid division by zero\n",
    "4. `total_tickets`: Sum of all ticket types\n",
    "5. `has_streaming`: 1 if customer has streaming_tv OR streaming_movies\n",
    "6. `has_protection`: 1 if customer has online_security AND tech_support\n",
    "\n",
    "💡 **Hint:**\n",
    "```python\n",
    "df['tenure_years'] = df['tenure_months'] / 12\n",
    "df['avg_monthly_charges'] = df['total_charges'] / df['tenure_months']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 2: YOUR CODE HERE\n",
    "# Create domain knowledge features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✅ **Check:** Do your new features make business sense? \n",
    "- tenure_years should be between 0 and 6\n",
    "- avg_monthly_charges should be close to monthly_charges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create Mathematical Features\n",
    "\n",
    "Create new features through mathematical combinations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 3: Create Mathematical Features\n",
    "\n",
    "Create ratio and product features:\n",
    "1. `charges_to_tenure_ratio`: total_charges / tenure_months\n",
    "2. `data_per_session`: data_usage_gb / (avg_session_duration_min + 1)\n",
    "3. `support_intensity`: num_support_calls / tenure_months\n",
    "4. `total_service_usage`: avg_session_duration_min * data_usage_gb\n",
    "\n",
    "💡 **Hint:** Add small constants to avoid division by zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 3: YOUR CODE HERE\n",
    "# Create mathematical features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Extract Datetime Features\n",
    "\n",
    "Dates contain rich information - let's extract it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 4: Extract Datetime Features\n",
    "\n",
    "From `signup_date` and `last_payment_date`, extract:\n",
    "1. `signup_month`: Month of signup (1-12)\n",
    "2. `signup_day_of_week`: Day of week (0=Monday, 6=Sunday)\n",
    "3. `signup_quarter`: Quarter of year (1-4)\n",
    "4. `is_signup_weekend`: 1 if signup was on weekend\n",
    "5. `days_since_signup`: Days between signup and reference date (2024-01-01)\n",
    "6. `days_since_last_payment`: Days between last payment and reference date\n",
    "\n",
    "💡 **Hint:**\n",
    "```python\n",
    "df['signup_month'] = pd.to_datetime(df['signup_date']).dt.month\n",
    "df['signup_day_of_week'] = pd.to_datetime(df['signup_date']).dt.dayofweek\n",
    "df['signup_quarter'] = pd.to_datetime(df['signup_date']).dt.quarter\n",
    "df['is_signup_weekend'] = (df['signup_day_of_week'] >= 5).astype(int)\n",
    "reference_date = pd.Timestamp('2024-01-01')\n",
    "df['days_since_signup'] = (reference_date - pd.to_datetime(df['signup_date'])).dt.days\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 4: YOUR CODE HERE\n",
    "# Extract datetime features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Apply Log Transformation\n",
    "\n",
    "Log transformations help normalize skewed distributions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 5: Apply Log Transformation to Skewed Features\n",
    "\n",
    "1. Identify skewed numerical features (skewness > 1)\n",
    "2. Apply log transformation: `log1p` (log(1+x) to handle zeros)\n",
    "3. Visualize before and after distributions\n",
    "\n",
    "Focus on: `total_charges`, `data_usage_gb`, `avg_session_duration_min`\n",
    "\n",
    "💡 **Hint:**\n",
    "```python\n",
    "# Check skewness\n",
    "print(df['total_charges'].skew())\n",
    "\n",
    "# Apply log transformation\n",
    "df['total_charges_log'] = np.log1p(df['total_charges'])\n",
    "df['data_usage_log'] = np.log1p(df['data_usage_gb'])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 5: YOUR CODE HERE\n",
    "# Apply log transformation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 6: Apply Box-Cox Transformation\n",
    "\n",
    "Box-Cox is a more sophisticated transformation that finds optimal power:\n",
    "1. Apply to `monthly_charges`\n",
    "2. Compare with log transformation\n",
    "3. Visualize both\n",
    "\n",
    "💡 **Hint:**\n",
    "```python\n",
    "from scipy.stats import boxcox\n",
    "df['monthly_charges_boxcox'], lambda_param = boxcox(df['monthly_charges'] + 1)\n",
    "print(f\"Optimal lambda: {lambda_param:.4f}\")\n",
    "```\n",
    "\n",
    "⚠️ **Common Mistake:** Box-Cox requires all positive values! Add 1 if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 6: YOUR CODE HERE\n",
    "# Apply Box-Cox transformation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Normalize Numerical Features\n",
    "\n",
    "Scaling ensures all features have similar ranges!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 7: Apply StandardScaler and MinMaxScaler\n",
    "\n",
    "Compare two scaling methods:\n",
    "1. **StandardScaler**: (x - mean) / std → mean=0, std=1\n",
    "2. **MinMaxScaler**: (x - min) / (max - min) → range [0, 1]\n",
    "\n",
    "Apply to: `age`, `tenure_months`, `monthly_charges`, `data_usage_gb`\n",
    "\n",
    "💡 **Hint:**\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# StandardScaler\n",
    "scaler_std = StandardScaler()\n",
    "cols_to_scale = ['age', 'tenure_months', 'monthly_charges', 'data_usage_gb']\n",
    "df[['age_std', 'tenure_std', 'charges_std', 'data_std']] = scaler_std.fit_transform(df[cols_to_scale])\n",
    "\n",
    "# MinMaxScaler\n",
    "scaler_minmax = MinMaxScaler()\n",
    "df[['age_minmax', 'tenure_minmax', 'charges_minmax', 'data_minmax']] = scaler_minmax.fit_transform(df[cols_to_scale])\n",
    "```\n",
    "\n",
    "⚠️ **Important:** In real ML pipelines, fit on training data only!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 7: YOUR CODE HERE\n",
    "# Apply normalization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: One-Hot Encode Categorical Variables\n",
    "\n",
    "Convert categorical variables into binary columns!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 8: One-Hot Encode Low-Cardinality Categoricals\n",
    "\n",
    "One-hot encode these features:\n",
    "- `contract_type` (3 values)\n",
    "- `internet_service` (3 values)\n",
    "\n",
    "💡 **Hint:**\n",
    "```python\n",
    "# Using pandas\n",
    "df_encoded = pd.get_dummies(df, columns=['contract_type', 'internet_service'], prefix=['contract', 'internet'])\n",
    "\n",
    "# OR using sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')  # drop='first' avoids multicollinearity\n",
    "encoded = encoder.fit_transform(df[['contract_type', 'internet_service']])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 8: YOUR CODE HERE\n",
    "# One-hot encode categorical variables\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✅ **Check:** How many new columns were created? For n categories, you get n or n-1 columns (if drop='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Label Encode Ordinal Variables\n",
    "\n",
    "For categories with natural order, use label encoding!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 9: Label Encode with Meaningful Order\n",
    "\n",
    "Create an ordinal feature for `contract_type` with meaningful order:\n",
    "- Month-to-month → 0 (least commitment)\n",
    "- One year → 1 (medium commitment)\n",
    "- Two year → 2 (most commitment)\n",
    "\n",
    "💡 **Hint:**\n",
    "```python\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "contract_order = [['Month-to-month', 'One year', 'Two year']]\n",
    "ord_encoder = OrdinalEncoder(categories=contract_order)\n",
    "df['contract_ordinal'] = ord_encoder.fit_transform(df[['contract_type']])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 9: YOUR CODE HERE\n",
    "# Label encode ordinal variables\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Target Encoding (Carefully!)\n",
    "\n",
    "Encode categories using target mean - powerful but requires care to avoid leakage!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 10: Apply Target Encoding\n",
    "\n",
    "Target encode `payment_method` using churn rate:\n",
    "1. Calculate mean churn rate per payment method\n",
    "2. Replace categories with their mean target value\n",
    "3. Add smoothing to avoid overfitting\n",
    "\n",
    "💡 **Hint:**\n",
    "```python\n",
    "# Calculate mean target per category\n",
    "target_encoding = df.groupby('payment_method')['churn'].mean()\n",
    "df['payment_method_target_enc'] = df['payment_method'].map(target_encoding)\n",
    "\n",
    "# Add smoothing (blend with global mean)\n",
    "global_mean = df['churn'].mean()\n",
    "counts = df['payment_method'].value_counts()\n",
    "smoothing = 10\n",
    "smooth_encoding = (target_encoding * counts + global_mean * smoothing) / (counts + smoothing)\n",
    "df['payment_method_smooth_enc'] = df['payment_method'].map(smooth_encoding)\n",
    "```\n",
    "\n",
    "⚠️ **Warning:** In real ML, do this only on training set, then apply to test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 10: YOUR CODE HERE\n",
    "# Apply target encoding\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Frequency Encoding\n",
    "\n",
    "Encode by how often each category appears!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 11: Apply Frequency Encoding\n",
    "\n",
    "Encode `payment_method` by frequency:\n",
    "1. Count occurrences of each category\n",
    "2. Replace category with its frequency\n",
    "\n",
    "💡 **Hint:**\n",
    "```python\n",
    "freq_encoding = df['payment_method'].value_counts()\n",
    "df['payment_method_freq'] = df['payment_method'].map(freq_encoding)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 11: YOUR CODE HERE\n",
    "# Apply frequency encoding\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Create Binned Features\n",
    "\n",
    "Sometimes continuous variables work better as categories!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 12: Create Binned Features\n",
    "\n",
    "Bin continuous features into categories:\n",
    "1. **Equal width bins**: `pd.cut()` - divide range into equal intervals\n",
    "2. **Equal frequency bins**: `pd.qcut()` - divide into quantiles\n",
    "\n",
    "Apply to `age` and `monthly_charges`:\n",
    "- Age: Young (18-35), Middle (36-55), Senior (56+)\n",
    "- Charges: Low, Medium, High, Very High (quartiles)\n",
    "\n",
    "💡 **Hint:**\n",
    "```python\n",
    "# Equal width bins\n",
    "df['age_bin'] = pd.cut(df['age'], bins=[0, 35, 55, 100], labels=['Young', 'Middle', 'Senior'])\n",
    "\n",
    "# Equal frequency bins (quartiles)\n",
    "df['charges_quartile'] = pd.qcut(df['monthly_charges'], q=4, labels=['Low', 'Medium', 'High', 'VeryHigh'])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 12: YOUR CODE HERE\n",
    "# Create binned features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 14: Create Interaction Features\n",
    "\n",
    "Capture relationships between features!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 13: Create Interaction Features\n",
    "\n",
    "Create meaningful interactions:\n",
    "\n",
    "**Manual interactions:**\n",
    "1. `tenure_x_charges`: tenure_months * monthly_charges\n",
    "2. `is_fiber_and_streaming`: 1 if Fiber optic AND has streaming\n",
    "3. `senior_no_support`: 1 if age>60 AND tech_support='No'\n",
    "\n",
    "**Polynomial features:**\n",
    "4. Use PolynomialFeatures for automated interactions\n",
    "\n",
    "💡 **Hint:**\n",
    "```python\n",
    "# Manual\n",
    "df['tenure_x_charges'] = df['tenure_months'] * df['monthly_charges']\n",
    "\n",
    "# Polynomial (creates all interactions + squares)\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "features_to_interact = ['tenure_months', 'monthly_charges', 'num_support_calls']\n",
    "interactions = poly.fit_transform(df[features_to_interact])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 13: YOUR CODE HERE\n",
    "# Create interaction features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 15: Build Complete Feature Engineering Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 14: Create Automated Feature Engineering Pipeline\n",
    "\n",
    "Build a pipeline that combines:\n",
    "1. Numerical transformations (log, scaling)\n",
    "2. Categorical encoding (one-hot)\n",
    "3. Feature selection (optional)\n",
    "\n",
    "Use ColumnTransformer to handle different feature types!\n",
    "\n",
    "💡 **Hint:**\n",
    "```python\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Define transformers for different column types\n",
    "numerical_features = ['age', 'tenure_months', 'monthly_charges']\n",
    "categorical_features = ['contract_type', 'internet_service']\n",
    "\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(drop='first', handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 14: YOUR CODE HERE\n",
    "# Build complete feature engineering pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 16: Evaluate Impact of Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 15: Compare Model Performance\n",
    "\n",
    "Train models and compare:\n",
    "1. **Baseline**: Original features only\n",
    "2. **Engineered**: With all your new features\n",
    "\n",
    "Use cross-validation for robust comparison!\n",
    "\n",
    "💡 **Hint:**\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Prepare baseline features\n",
    "baseline_features = ['age', 'tenure_months', 'monthly_charges', 'total_charges', \n",
    "                     'num_support_calls', 'num_tech_tickets']\n",
    "\n",
    "# Prepare engineered features (select your best ones)\n",
    "engineered_features = baseline_features + [\n",
    "    'tenure_years', 'avg_monthly_charges', 'total_tickets',\n",
    "    'charges_to_tenure_ratio', 'support_intensity',\n",
    "    'total_charges_log', 'data_usage_log',\n",
    "    # ... add your engineered features\n",
    "]\n",
    "\n",
    "# Encode categoricals if needed\n",
    "le = LabelEncoder()\n",
    "for col in df.select_dtypes(include=['object', 'category']).columns:\n",
    "    if col in df.columns:\n",
    "        df[col + '_encoded'] = le.fit_transform(df[col].astype(str))\n",
    "\n",
    "# Train and compare\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "scores_baseline = cross_val_score(clf, df[baseline_features], df['churn'], cv=5)\n",
    "scores_engineered = cross_val_score(clf, df[engineered_features], df['churn'], cv=5)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 15: YOUR CODE HERE\n",
    "# Compare model performance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 17: Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 16: Analyze Feature Importance\n",
    "\n",
    "1. Train a RandomForest on engineered features\n",
    "2. Extract feature importances\n",
    "3. Plot top 15 most important features\n",
    "4. Identify which engineered features are most valuable\n",
    "\n",
    "💡 **Hint:**\n",
    "```python\n",
    "# Train model\n",
    "X = df[engineered_features]\n",
    "y = df['churn']\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Get feature importances\n",
    "importances = clf.feature_importances_\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': engineered_features,\n",
    "    'importance': importances\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_features = feature_importance_df.head(15)\n",
    "plt.barh(top_features['feature'], top_features['importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top 15 Most Important Features')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 16: YOUR CODE HERE\n",
    "# Analyze feature importance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "### You Did It!\n",
    "\n",
    "You just:\n",
    "- ✅ Created domain knowledge features\n",
    "- ✅ Applied mathematical transformations\n",
    "- ✅ Extracted datetime features\n",
    "- ✅ Normalized and scaled features\n",
    "- ✅ Encoded categorical variables (5 methods!)\n",
    "- ✅ Created binned features\n",
    "- ✅ Built interaction features\n",
    "- ✅ Constructed automated pipeline\n",
    "- ✅ Evaluated feature engineering impact\n",
    "- ✅ Analyzed feature importance\n",
    "\n",
    "### What You Learned:\n",
    "\n",
    "**1. Feature Creation:**\n",
    "- Domain knowledge features capture business logic\n",
    "- Mathematical features reveal hidden relationships\n",
    "- Datetime features extract temporal patterns\n",
    "\n",
    "**2. Feature Transformation:**\n",
    "- Log transforms reduce skewness\n",
    "- Box-Cox finds optimal transformation\n",
    "- Scaling ensures equal feature importance\n",
    "\n",
    "**3. Encoding Strategies:**\n",
    "- One-hot: Best for nominal categories (no order)\n",
    "- Ordinal: Use when categories have natural order\n",
    "- Target: Powerful but risk of leakage\n",
    "- Frequency: Simple and effective\n",
    "- Binning: Convert continuous to categorical\n",
    "\n",
    "**4. Feature Interactions:**\n",
    "- Manual interactions use domain knowledge\n",
    "- Polynomial features automate discovery\n",
    "- Captures non-linear relationships\n",
    "\n",
    "**5. Best Practices:**\n",
    "- Always split data BEFORE engineering (avoid leakage)\n",
    "- Use pipelines for reproducibility\n",
    "- Validate improvements with cross-validation\n",
    "- Remove low-importance features\n",
    "- Document your feature engineering decisions\n",
    "\n",
    "### Key Insights:\n",
    "- Good features > complex models\n",
    "- Feature engineering is creative and iterative\n",
    "- Domain knowledge is invaluable\n",
    "- Not all engineered features help - test them!\n",
    "- Pipelines make feature engineering reproducible\n",
    "\n",
    "### Next Steps:\n",
    "- Try Lab 3: Class Imbalance Project\n",
    "- Apply to your own datasets\n",
    "- Experiment with more advanced techniques\n",
    "\n",
    "---\n",
    "\n",
    "## Extension Exercises (Optional, Harder!)\n",
    "\n",
    "1. **Automated Feature Engineering**: Try `featuretools` library\n",
    "2. **Feature Selection**: Implement RFE (Recursive Feature Elimination)\n",
    "3. **Time-Based Features**: Create lag features, rolling averages\n",
    "4. **Text Features**: If you had text data, try TF-IDF, word embeddings\n",
    "5. **Embeddings**: For high-cardinality categoricals, try entity embeddings\n",
    "6. **Genetic Programming**: Use TPOT for automated feature engineering\n",
    "\n",
    "---\n",
    "\n",
    "## You're a Feature Engineering Master Now!\n",
    "\n",
    "**You just mastered the art of feature engineering!**\n",
    "\n",
    "**That's AMAZING! Keep engineering those features!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Solutions (Only Look After Trying!)\n",
    "\n",
    "Here are the solutions to all TODOs. But remember: **you learn by doing, not by copying!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION TO TODO 1\n",
    "print(\"First 10 rows:\")\n",
    "print(df.head(10))\n",
    "\n",
    "print(\"\\nDataset Info:\")\n",
    "print(df.info())\n",
    "\n",
    "numerical_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = df.select_dtypes(include=['object']).columns.tolist()\n",
    "datetime_features = df.select_dtypes(include=['datetime64']).columns.tolist()\n",
    "\n",
    "print(f\"\\nFeature Types:\")\n",
    "print(f\"Numerical features ({len(numerical_features)}): {numerical_features}\")\n",
    "print(f\"Categorical features ({len(categorical_features)}): {categorical_features}\")\n",
    "print(f\"Datetime features ({len(datetime_features)}): {datetime_features}\")\n",
    "\n",
    "print(\"\\nBasic Statistics:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION TO TODO 2\n",
    "print(\"Creating domain knowledge features...\")\n",
    "\n",
    "# 1. Tenure in years\n",
    "df['tenure_years'] = df['tenure_months'] / 12\n",
    "\n",
    "# 2. Average monthly charges\n",
    "df['avg_monthly_charges'] = df['total_charges'] / df['tenure_months']\n",
    "\n",
    "# 3. Charges per support call\n",
    "df['charges_per_call'] = df['monthly_charges'] / (df['num_support_calls'] + 1)\n",
    "\n",
    "# 4. Total tickets\n",
    "df['total_tickets'] = df['num_tech_tickets'] + df['num_admin_tickets'] + df['num_support_calls']\n",
    "\n",
    "# 5. Has streaming services\n",
    "df['has_streaming'] = ((df['streaming_tv'] == 'Yes') | (df['streaming_movies'] == 'Yes')).astype(int)\n",
    "\n",
    "# 6. Has protection\n",
    "df['has_protection'] = ((df['online_security'] == 'Yes') & (df['tech_support'] == 'Yes')).astype(int)\n",
    "\n",
    "print(\"✅ Created 6 domain knowledge features\")\n",
    "print(\"\\nSample values:\")\n",
    "print(df[['tenure_months', 'tenure_years', 'monthly_charges', 'avg_monthly_charges', \n",
    "          'total_tickets', 'has_streaming', 'has_protection']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION TO TODO 3\n",
    "print(\"Creating mathematical features...\")\n",
    "\n",
    "# 1. Charges to tenure ratio\n",
    "df['charges_to_tenure_ratio'] = df['total_charges'] / (df['tenure_months'] + 1)\n",
    "\n",
    "# 2. Old per session\n",
    "df['data_per_session'] = df['data_usage_gb'] / (df['avg_session_duration_min'] + 1)\n",
    "\n",
    "# 3. Support intensity\n",
    "df['support_intensity'] = df['num_support_calls'] / (df['tenure_months'] + 1)\n",
    "\n",
    "# 4. Total service usage\n",
    "df['total_service_usage'] = df['avg_session_duration_min'] * df['data_usage_gb']\n",
    "\n",
    "print(\"✅ Created 4 mathematical features\")\n",
    "print(\"\\nSample values:\")\n",
    "print(df[['charges_to_tenure_ratio', 'data_per_session', 'support_intensity', 'total_service_usage']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION TO TODO 4\n",
    "print(\"Extracting datetime features...\")\n",
    "\n",
    "# Convert to datetime if needed\n",
    "df['signup_date'] = pd.to_datetime(df['signup_date'])\n",
    "df['last_payment_date'] = pd.to_datetime(df['last_payment_date'])\n",
    "\n",
    "# 1. Signup month\n",
    "df['signup_month'] = df['signup_date'].dt.month\n",
    "\n",
    "# 2. Signup day of week\n",
    "df['signup_day_of_week'] = df['signup_date'].dt.dayofweek\n",
    "\n",
    "# 3. Signup quarter\n",
    "df['signup_quarter'] = df['signup_date'].dt.quarter\n",
    "\n",
    "# 4. Is signup weekend\n",
    "df['is_signup_weekend'] = (df['signup_day_of_week'] >= 5).astype(int)\n",
    "\n",
    "# 5. Days since signup\n",
    "reference_date = pd.Timestamp('2024-01-01')\n",
    "df['days_since_signup'] = (reference_date - df['signup_date']).dt.days\n",
    "\n",
    "# 6. Days since last payment\n",
    "df['days_since_last_payment'] = (reference_date - df['last_payment_date']).dt.days\n",
    "\n",
    "print(\"✅ Created 6 datetime features\")\n",
    "print(\"\\nSample values:\")\n",
    "print(df[['signup_date', 'signup_month', 'signup_day_of_week', 'signup_quarter', \n",
    "          'is_signup_weekend', 'days_since_signup', 'days_since_last_payment']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION TO TODO 5\n",
    "print(\"Applying log transformation...\\n\")\n",
    "\n",
    "# Check skewness first\n",
    "features_to_transform = ['total_charges', 'data_usage_gb', 'avg_session_duration_min']\n",
    "print(\"Skewness before transformation:\")\n",
    "for col in features_to_transform:\n",
    "    print(f\"  {col}: {df[col].skew():.2f}\")\n",
    "\n",
    "# Apply log transformation\n",
    "df['total_charges_log'] = np.log1p(df['total_charges'])\n",
    "df['data_usage_log'] = np.log1p(df['data_usage_gb'])\n",
    "df['session_duration_log'] = np.log1p(df['avg_session_duration_min'])\n",
    "\n",
    "print(\"\\nSkewness after log transformation:\")\n",
    "print(f\"  total_charges_log: {df['total_charges_log'].skew():.2f}\")\n",
    "print(f\"  data_usage_log: {df['data_usage_log'].skew():.2f}\")\n",
    "print(f\"  session_duration_log: {df['session_duration_log'].skew():.2f}\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(3, 2, figsize=(14, 12))\n",
    "for idx, col in enumerate(features_to_transform):\n",
    "    # Before\n",
    "    axes[idx, 0].hist(df[col], bins=50, alpha=0.7, color='red', edgecolor='black')\n",
    "    axes[idx, 0].set_title(f'Before: {col}')\n",
    "    axes[idx, 0].set_ylabel('Frequency')\n",
    "    \n",
    "    # After\n",
    "    log_col = col.replace('total_charges', 'total_charges_log').replace('data_usage_gb', 'data_usage_log').replace('avg_session_duration_min', 'session_duration_log')\n",
    "    axes[idx, 1].hist(df[log_col], bins=50, alpha=0.7, color='green', edgecolor='black')\n",
    "    axes[idx, 1].set_title(f'After: {log_col}')\n",
    "    axes[idx, 1].set_ylabel('Frequency')\n",
    "\n",
    "plt.suptitle('Log Transformation: Before vs After', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✅ Log transformation reduces skewness!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION TO TODO 6\n",
    "from scipy.stats import boxcox\n",
    "\n",
    "print(\"Applying Box-Cox transformation...\\n\")\n",
    "\n",
    "# Box-Cox requires positive values\n",
    "df['monthly_charges_boxcox'], lambda_param = boxcox(df['monthly_charges'] + 1)\n",
    "df['monthly_charges_log'] = np.log1p(df['monthly_charges'])\n",
    "\n",
    "print(f\"Optimal Box-Cox lambda: {lambda_param:.4f}\")\n",
    "print(f\"\\nSkewness comparison:\")\n",
    "print(f\"  Original: {df['monthly_charges'].skew():.4f}\")\n",
    "print(f\"  Log: {df['monthly_charges_log'].skew():.4f}\")\n",
    "print(f\"  Box-Cox: {df['monthly_charges_boxcox'].skew():.4f}\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "axes[0].hist(df['monthly_charges'], bins=50, alpha=0.7, color='red', edgecolor='black')\n",
    "axes[0].set_title('Original')\n",
    "axes[1].hist(df['monthly_charges_log'], bins=50, alpha=0.7, color='blue', edgecolor='black')\n",
    "axes[1].set_title('Log Transform')\n",
    "axes[2].hist(df['monthly_charges_boxcox'], bins=50, alpha=0.7, color='green', edgecolor='black')\n",
    "axes[2].set_title(f'Box-Cox (λ={lambda_param:.2f})')\n",
    "plt.suptitle('Comparison: Log vs Box-Cox Transformation', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✅ Box-Cox finds optimal transformation automatically!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION TO TODO 7\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "print(\"Applying normalization...\\n\")\n",
    "\n",
    "cols_to_scale = ['age', 'tenure_months', 'monthly_charges', 'data_usage_gb']\n",
    "\n",
    "# StandardScaler\n",
    "scaler_std = StandardScaler()\n",
    "scaled_std = scaler_std.fit_transform(df[cols_to_scale])\n",
    "df[['age_std', 'tenure_std', 'charges_std', 'data_std']] = scaled_std\n",
    "\n",
    "# MinMaxScaler\n",
    "scaler_minmax = MinMaxScaler()\n",
    "scaled_minmax = scaler_minmax.fit_transform(df[cols_to_scale])\n",
    "df[['age_minmax', 'tenure_minmax', 'charges_minmax', 'data_minmax']] = scaled_minmax\n",
    "\n",
    "print(\"StandardScaler results (mean≈0, std≈1):\")\n",
    "print(df[['age_std', 'tenure_std', 'charges_std', 'data_std']].describe())\n",
    "\n",
    "print(\"\\nMinMaxScaler results (range [0,1]):\")\n",
    "print(df[['age_minmax', 'tenure_minmax', 'charges_minmax', 'data_minmax']].describe())\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "test_col = 'age'\n",
    "axes[0].hist(df[test_col], bins=30, alpha=0.7, color='red')\n",
    "axes[0].set_title('Original')\n",
    "axes[1].hist(df[test_col + '_std'], bins=30, alpha=0.7, color='blue')\n",
    "axes[1].set_title('StandardScaler')\n",
    "axes[2].hist(df[test_col + '_minmax'], bins=30, alpha=0.7, color='green')\n",
    "axes[2].set_title('MinMaxScaler')\n",
    "plt.suptitle(f'Normalization Comparison: {test_col}', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✅ Both scalers transform features to comparable ranges!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION TO TODO 8\n",
    "print(\"One-hot encoding categorical variables...\\n\")\n",
    "\n",
    "# Using pandas get_dummies\n",
    "df_encoded = pd.get_dummies(df, columns=['contract_type', 'internet_service'], \n",
    "                            prefix=['contract', 'internet'], drop_first=True)\n",
    "\n",
    "print(\"One-hot encoded columns created:\")\n",
    "contract_cols = [col for col in df_encoded.columns if col.startswith('contract_')]\n",
    "internet_cols = [col for col in df_encoded.columns if col.startswith('internet_')]\n",
    "print(f\"  Contract: {contract_cols}\")\n",
    "print(f\"  Internet: {internet_cols}\")\n",
    "\n",
    "print(f\"\\nShape before: {df.shape}\")\n",
    "print(f\"Shape after: {df_encoded.shape}\")\n",
    "print(f\"New columns added: {df_encoded.shape[1] - df.shape[1]}\")\n",
    "\n",
    "print(\"\\nSample encoded values:\")\n",
    "print(df_encoded[contract_cols + internet_cols].head())\n",
    "\n",
    "print(\"\\n✅ One-hot encoding complete! (drop_first=True avoids multicollinearity)\")\n",
    "\n",
    "# Update df to include encoded columns\n",
    "df = df_encoded.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION TO TODO 9\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "print(\"Label encoding ordinal variables...\\n\")\n",
    "\n",
    "# Create a mapping with meaningful order\n",
    "contract_mapping = {\n",
    "    'Month-to-month': 0,\n",
    "    'One year': 1,\n",
    "    'Two year': 2\n",
    "}\n",
    "\n",
    "# Since we already one-hot encoded, let's create from original if available\n",
    "# For demo, recreate the original contract_type column\n",
    "if 'contract_Month-to-month' in df.columns:\n",
    "    # Reconstruct original for demo\n",
    "    df.loc[:, 'contract_type_ordinal'] = 0  # default\n",
    "    if 'contract_One year' in df.columns:\n",
    "        df.loc[df['contract_One year'] == 1, 'contract_type_ordinal'] = 1\n",
    "    if 'contract_Two year' in df.columns:\n",
    "        df.loc[df['contract_Two year'] == 1, 'contract_type_ordinal'] = 2\n",
    "else:\n",
    "    # Direct mapping if original still exists\n",
    "    df['contract_type_ordinal'] = df['contract_type'].map(contract_mapping)\n",
    "\n",
    "print(\"Contract type ordinal encoding:\")\n",
    "print(\"  Month-to-month → 0 (least commitment)\")\n",
    "print(\"  One year → 1 (medium commitment)\")\n",
    "print(\"  Two year → 2 (most commitment)\")\n",
    "\n",
    "print(\"\\nValue distribution:\")\n",
    "print(df['contract_type_ordinal'].value_counts().sort_index())\n",
    "\n",
    "print(\"\\n✅ Ordinal encoding preserves natural order!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION TO TODO 10\n",
    "print(\"Applying target encoding...\\n\")\n",
    "\n",
    "# Calculate mean churn rate per payment method\n",
    "target_encoding = df.groupby('payment_method')['churn'].mean()\n",
    "print(\"Mean churn rate by payment method:\")\n",
    "print(target_encoding.sort_values(ascending=False))\n",
    "\n",
    "# Simple target encoding\n",
    "df['payment_method_target_enc'] = df['payment_method'].map(target_encoding)\n",
    "\n",
    "# Smoothed target encoding (to avoid overfitting)\n",
    "global_mean = df['churn'].mean()\n",
    "counts = df['payment_method'].value_counts()\n",
    "smoothing = 10  # smoothing parameter\n",
    "\n",
    "smooth_encoding = (target_encoding * counts + global_mean * smoothing) / (counts + smoothing)\n",
    "df['payment_method_smooth_enc'] = df['payment_method'].map(smooth_encoding)\n",
    "\n",
    "print(\"\\nComparison: Simple vs Smoothed encoding:\")\n",
    "comparison = pd.DataFrame({\n",
    "    'Simple': target_encoding,\n",
    "    'Smoothed': smooth_encoding,\n",
    "    'Count': counts\n",
    "})\n",
    "print(comparison)\n",
    "\n",
    "print(\"\\n⚠️ IMPORTANT: In real ML, compute on training set only, then apply to test set!\")\n",
    "print(\"✅ Target encoding captures target relationship in categorical variables!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION TO TODO 11\n",
    "print(\"Applying frequency encoding...\\n\")\n",
    "\n",
    "# Calculate frequency of each category\n",
    "freq_encoding = df['payment_method'].value_counts()\n",
    "df['payment_method_freq'] = df['payment_method'].map(freq_encoding)\n",
    "\n",
    "print(\"Frequency by payment method:\")\n",
    "print(freq_encoding)\n",
    "\n",
    "print(\"\\nEncoded values sample:\")\n",
    "print(df[['payment_method', 'payment_method_freq']].head(10))\n",
    "\n",
    "print(\"\\n✅ Frequency encoding is simple and captures category prevalence!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION TO TODO 12\n",
    "print(\"Creating binned features...\\n\")\n",
    "\n",
    "# 1. Equal width bins for age\n",
    "df['age_bin'] = pd.cut(df['age'], bins=[0, 35, 55, 100], labels=['Young', 'Middle', 'Senior'])\n",
    "\n",
    "# 2. Equal frequency bins for monthly charges (quartiles)\n",
    "df['charges_quartile'] = pd.qcut(df['monthly_charges'], q=4, labels=['Low', 'Medium', 'High', 'VeryHigh'])\n",
    "\n",
    "print(\"Age bins (equal width):\")\n",
    "print(df['age_bin'].value_counts().sort_index())\n",
    "\n",
    "print(\"\\nCharges quartiles (equal frequency):\")\n",
    "print(df['charges_quartile'].value_counts().sort_index())\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "df['age_bin'].value_counts().sort_index().plot(kind='bar', ax=axes[0], color='steelblue')\n",
    "axes[0].set_title('Age Bins (Equal Width)')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_xlabel('Age Group')\n",
    "\n",
    "df['charges_quartile'].value_counts().sort_index().plot(kind='bar', ax=axes[1], color='coral')\n",
    "axes[1].set_title('Charges Quartiles (Equal Frequency)')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_xlabel('Charge Level')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✅ Binning converts continuous features to categorical!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION TO TODO 13\n",
    "print(\"Creating interaction features...\\n\")\n",
    "\n",
    "# Manual interactions\n",
    "df['tenure_x_charges'] = df['tenure_months'] * df['monthly_charges']\n",
    "\n",
    "df['is_fiber_and_streaming'] = (\n",
    "    (df['internet_Fiber optic'] if 'internet_Fiber optic' in df.columns else (df['internet_service'] == 'Fiber optic').astype(int)) & \n",
    "    (df['has_streaming'] == 1)\n",
    ").astype(int)\n",
    "\n",
    "df['senior_no_support'] = (\n",
    "    (df['age'] > 60) & \n",
    "    (df['tech_support'] == 'No')\n",
    ").astype(int)\n",
    "\n",
    "print(\"Manual interactions created:\")\n",
    "print(f\"  tenure_x_charges: {df['tenure_x_charges'].describe()}\")\n",
    "print(f\"  is_fiber_and_streaming: {df['is_fiber_and_streaming'].sum()} customers\")\n",
    "print(f\"  senior_no_support: {df['senior_no_support'].sum()} customers\")\n",
    "\n",
    "# Polynomial features (automated)\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "features_to_interact = ['tenure_months', 'monthly_charges', 'num_support_calls']\n",
    "interactions = poly.fit_transform(df[features_to_interact])\n",
    "interaction_names = poly.get_feature_names_out(features_to_interact)\n",
    "\n",
    "print(f\"\\nPolynomial interactions created: {len(interaction_names)} features\")\n",
    "print(f\"Feature names: {list(interaction_names)}\")\n",
    "\n",
    "# Add to dataframe\n",
    "for i, name in enumerate(interaction_names):\n",
    "    df[f'poly_{name}'] = interactions[:, i]\n",
    "\n",
    "print(\"\\n✅ Interaction features capture relationships between variables!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION TO TODO 14\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "print(\"Building feature engineering pipeline...\\n\")\n",
    "\n",
    "# Define feature groups\n",
    "numerical_features = ['age', 'tenure_months', 'monthly_charges', 'total_charges',\n",
    "                     'num_support_calls', 'num_tech_tickets', 'data_usage_gb']\n",
    "\n",
    "categorical_features = ['payment_method', 'online_security', 'tech_support',\n",
    "                       'streaming_tv', 'streaming_movies', 'paperless_billing']\n",
    "\n",
    "# Create transformers\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# Combine transformers\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='drop'  # Drop columns not specified\n",
    ")\n",
    "\n",
    "# Create full pipeline with model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "full_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "print(\"Pipeline structure:\")\n",
    "print(full_pipeline)\n",
    "\n",
    "print(\"\\n✅ Pipeline is ready! It will:\")\n",
    "print(\"   1. Scale numerical features\")\n",
    "print(\"   2. One-hot encode categorical features\")\n",
    "print(\"   3. Train a RandomForest classifier\")\n",
    "print(\"\\nThis ensures consistent preprocessing on train and test data!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION TO TODO 15\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Comparing model performance: Baseline vs Engineered Features\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Prepare features\n",
    "baseline_features = ['age', 'tenure_months', 'monthly_charges', 'total_charges',\n",
    "                    'num_support_calls', 'num_tech_tickets']\n",
    "\n",
    "engineered_features = baseline_features + [\n",
    "    'tenure_years', 'avg_monthly_charges', 'total_tickets',\n",
    "    'charges_to_tenure_ratio', 'support_intensity',\n",
    "    'total_charges_log', 'data_usage_log',\n",
    "    'signup_month', 'signup_quarter', 'days_since_signup',\n",
    "    'charges_per_call', 'data_per_session', 'total_service_usage',\n",
    "    'contract_type_ordinal', 'payment_method_target_enc',\n",
    "    'tenure_x_charges', 'has_streaming', 'has_protection'\n",
    "]\n",
    "\n",
    "# Filter to ensure columns exist\n",
    "engineered_features = [f for f in engineered_features if f in df.columns]\n",
    "\n",
    "# Train models\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "\n",
    "print(\"Training baseline model...\")\n",
    "scores_baseline = cross_val_score(clf, df[baseline_features], df['churn'], cv=5, scoring='accuracy')\n",
    "\n",
    "print(\"Training engineered features model...\")\n",
    "scores_engineered = cross_val_score(clf, df[engineered_features], df['churn'], cv=5, scoring='accuracy')\n",
    "\n",
    "# Results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nBaseline Features ({len(baseline_features)} features):\")\n",
    "print(f\"  Mean Accuracy: {scores_baseline.mean():.4f} (+/- {scores_baseline.std():.4f})\")\n",
    "print(f\"  Scores: {[f'{s:.4f}' for s in scores_baseline]}\")\n",
    "\n",
    "print(f\"\\nEngineered Features ({len(engineered_features)} features):\")\n",
    "print(f\"  Mean Accuracy: {scores_engineered.mean():.4f} (+/- {scores_engineered.std():.4f})\")\n",
    "print(f\"  Scores: {[f'{s:.4f}' for s in scores_engineered]}\")\n",
    "\n",
    "improvement = (scores_engineered.mean() - scores_baseline.mean()) * 100\n",
    "print(f\"\\n🎯 Improvement: {improvement:+.2f}% accuracy\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(['Baseline', 'Engineered'], \n",
    "        [scores_baseline.mean(), scores_engineered.mean()],\n",
    "        yerr=[scores_baseline.std(), scores_engineered.std()],\n",
    "        color=['coral', 'steelblue'], alpha=0.7, capsize=10)\n",
    "plt.ylabel('Cross-Validation Accuracy')\n",
    "plt.title('Model Performance: Baseline vs Engineered Features')\n",
    "plt.ylim([scores_baseline.mean()-0.05, scores_engineered.mean()+0.05])\n",
    "for i, (name, score) in enumerate([('Baseline', scores_baseline.mean()), ('Engineered', scores_engineered.mean())]):\n",
    "    plt.text(i, score+0.01, f'{score:.4f}', ha='center', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "if improvement > 0:\n",
    "    print(\"\\n✅ Feature engineering improved model performance!\")\n",
    "else:\n",
    "    print(\"\\n⚠️ Feature engineering didn't help. Try different features or remove redundant ones.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION TO TODO 16\n",
    "print(\"Analyzing feature importance...\\n\")\n",
    "\n",
    "# Train model on all engineered features\n",
    "X = df[engineered_features]\n",
    "y = df['churn']\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Get feature importances\n",
    "importances = clf.feature_importances_\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': engineered_features,\n",
    "    'importance': importances\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 20 Most Important Features:\")\n",
    "print(\"=\"*60)\n",
    "for idx, row in feature_importance_df.head(20).iterrows():\n",
    "    print(f\"{row['feature']:35s}: {row['importance']:.6f}\")\n",
    "\n",
    "# Identify engineered features in top 10\n",
    "top_10 = feature_importance_df.head(10)['feature'].tolist()\n",
    "engineered_in_top10 = [f for f in top_10 if f not in baseline_features]\n",
    "\n",
    "print(f\"\\n🎯 Engineered features in top 10: {len(engineered_in_top10)}\")\n",
    "if engineered_in_top10:\n",
    "    print(\"  \", engineered_in_top10)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 10))\n",
    "top_features = feature_importance_df.head(15)\n",
    "colors = ['steelblue' if f not in baseline_features else 'coral' for f in top_features['feature']]\n",
    "plt.barh(top_features['feature'], top_features['importance'], color=colors, alpha=0.7)\n",
    "plt.xlabel('Importance', fontsize=12)\n",
    "plt.title('Top 15 Most Important Features', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "# Add legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor='coral', alpha=0.7, label='Baseline Features'),\n",
    "                   Patch(facecolor='steelblue', alpha=0.7, label='Engineered Features')]\n",
    "plt.legend(handles=legend_elements, loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✅ Feature importance analysis complete!\")\n",
    "print(\"Blue bars = Engineered features, Orange bars = Baseline features\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
